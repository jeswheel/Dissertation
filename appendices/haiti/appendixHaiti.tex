\chapter{Appendix for Chapter 3}
\label{chpt:appendix_haiti}


\section{Model Diagrams}\label{sec:appendix_haiti_diagrams}

Each of the dynamic models considered in this manuscript can be fully described using the model descriptions in the manuscript, coupled with the additional information described in Sections 2 and 3 of this supplement.
Despite this, diagrams of dynamic systems are often helpful to understand the equations.
In this section, we give three diagrams representing Models~1--3, respectively.
Because the models are defined by their mathematical equations and numeric implementation, these diagrams are not unique visual representations of the model.
Alternative representations that may be helpful in understanding the models explored in this paper are provided in the supplement material of \citet{lee20}.

\input{appendices/haiti/mod1diagram}

\input{appendices/haiti/mod2diagram}

\input{appendices/haiti/mod3diagram}

\section{Markov chain and differential equation interpretations of compartment flow rates}\label{sec:appendix_haiti_details}

\input{appendices/haiti/modelDetails}



%%%%%%%%%% START

\section{Confidence Intervals for Model Parameters}\label{sec:appendix_haiti_ci}

In this section we provide confidence intervals for all model parameters, excluding those that take unique values for each spatial unit.
For each model and parameter, we use principles of profile likelihood to obtain confidence intervals \citep{pawitan01}.
Due to the non-linear and stochastic nature of Models~1 and 3, exact evaluations of the profile log-likelihood are difficult to obtain.
Instead, the log-likelihood at each point of the profile is estimated using via Monte-Carlo based particle filter methods.
We therefore obtain confidence intervals for the parameters of Model~1 and Model~3 using the Monte Carlo adjust profile (MCAP) algorithm \citep{ionides17}.

Profile confidence intervals for nonlinear POMP models are require a large number of computations. In the Model~1 and Model~3 subsections, we mention the total computational expense of each profile log-likelihood evaluation. Each subsection also provide figures that show the curvature of the profile log-likelihood near the MLE (Figures~\ref{fig:m1Profs}--\ref{fig:m3Profs}).
In these figures, the parameter values are shown on the transformed scale in which the profile was calculated.

\subsection{Model~1 parameters}

Parameter estimates for Model~1, along with the MCAP confidence intervals for the estimate, are given in Table~\ref{tab:mod1CI}.
Figure~\ref{fig:m1Profs} displays the Monte Carlo evaluations of the profile likelihood values, obtained using a particle filter.
The total computational burden of this profile likelihood search was 3631 hours, which was computed in parallel using 9675 separate jobs via the \texttt{batchtools} R package \cite{batchtools}.





\begin{figure}[ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/m1Profs-1} 
\end{knitrout}
\caption[MCAP confidence intervals for Model 1 parameters.]{\label{fig:m1Profs}MCAP confidence intervals for Model 1 parameters. The vertical blue line indicates the smoothed MLE.}
\end{figure}

\begin{table}[!h]
\centering
\caption[Model~1 parameter estimates and confidence intervals.]{\label{tab:mod1CI}Model~1 parameter estimates and their corresponding confidence intervals, obtained via the MCAP algorithm.}
\vspace{2mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Mechanism} & \textbf{Parameter} & \textbf{MLE} & $\bm{95\%}$ \textbf{Confidence Interval}
\\
\hline
\hline

 Seasonality & $\transmissionTrend$ & $-0.036$
   &
  $(-0.070, -0.008)$
\\
\hline

 Seasonality & $\transmission_1$ & $1.417$
   &
  $(1.277, 1.811)$
\\
\hline

 Seasonality & $\transmission_2$ & $1.169$
   &
  $(0.937, 1.445)$
\\
\hline

 Seasonality & $\transmission_3$ & $1.136$
   &
  $(0.990, 1.630)$
\\
\hline

 Seasonality & $\transmission_4$ & $1.140$
   &
  $(0.922, 1.389)$
\\
\hline

 Seasonality & $\transmission_5$ & $1.401$
   &
  $(1.261, 1.687)$
\\
\hline

 Seasonality & $\transmission_6$ & $0.988$
   &
  $(0.699, 1.132)$
\\
\hline

 Observation Variance & $\obsOverdispersion: \mathrm{Epi}$ & $279.147$
   &
  $(177.226, 990.191)$
\\
\hline

 Observation Variance & $\obsOverdispersion: \mathrm{End}$ & $78.326$
   &
  $(57.171, 204.654)$
\\
\hline

  Reporting Rate & $\reportRate$ & $0.679$
   &
  $(0.315, 0.761)$
\\
\hline

  Mixing Exponent & $\mixExponent$ & $0.978$
   &
  $(0.938, 0.999)$
\\
\hline

  Process noise {\small (wk\textsuperscript{1/2})} & $\sigmaProc: \mathrm{Epi}$ & $0.092$
   &
  $(0.085, 0.113)$
\\
\hline

  Process noise {\small (wk\textsuperscript{1/2})} & $\sigmaProc: \mathrm{End}$ & $0.118$
   &
  $(0.092, 0.179)$
\\
\hline

  Initial Values & $I_{0}(0)$ & $7298$
   &
  $(2572, \ensuremath{1.4415\times 10^{4}})$
\\
\hline

  Initial Values & $E_{0}(0)$ & $350$
   &
  $(1, \ensuremath{1.3671\times 10^{4}})$
\\
\hline

\end{tabular}
\end{table}

\subsection{Model~2 parameters}

Parameter estimates for Model~2, along with the profile likelihood confidence intervals for each estimate, are given in Table~\ref{tab:mod2CI}.
Figure~\ref{fig:m2Profs} displays the profile log-likelihood curve near the MLE.
In Table~\ref{tab:mod2CI}, the confidence interval for $\muRS^{-1}$, the duration of natural immunity due to cholera infection, is arbitrarily large (going to infinity).
This is possible because the parameter that was estimated was $\muRS$, and the true MLE for this parameter is zero (see Figure~\ref{fig:m2Profs}).
This suggests that the fitted model favors a regime where reinfection events are not possible.
Similarly, the MLE for the parameter $\transmission$, which controls the amount of cholera transmission from human to human, is zero.
Because Model~2 fails to describe the incidence data as well as a simple statistical benchmark, we must be careful to not interpret these results as evidence that reinfections and human-to-human infection events do not occur.
Instead, we may consider this as additional evidence of model mispecification.



\begin{figure}[ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/m2Profs-1} 
\end{knitrout}
\caption[MCAP confidence intervals for Model 2 parameters.]{\label{fig:m2Profs}MCAP confidence intervals for Model 2 parameters. The vertical blue line indicates the MLE.}
\end{figure}

\begin{table}[!h]
\centering
\caption[Model~2 parameter estimates and confidence intervals.]{\label{tab:mod2CI}Model~2 parameter estimates and their corresponding confidence intervals, obtained via profile likelihood.}
\vspace{2mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Mechanism} & \textbf{Parameter} & \textbf{MLE} & $\bm{95\%}$ \textbf{Confidence Interval}
\\
\hline
\hline

 Human to water shedding {\small (wk\textsuperscript{-1})} & $\Wshed$ & $179.2$
   &
  $(144.6, 229.4)$
\\
\hline

 Water to Human Infection {\small (yr\textsuperscript{-1})} & $\beta_W$ & $1.098$
   &
  $(1.067, 1.128)$
\\
\hline

 Observation Variance & $\obsOverdispersion$ & $1.319$
   &
  $(1.291, 1.347)$
\\
\hline

 Seasonality & $\phaseParm$ & $0.974$
   &
  $(7.127, 7.381)$
\\
\hline

Human to Human Infection {\small (yr\textsuperscript{-1})} & $\transmission$ & $\ensuremath{5.97\times 10^{-15}}$\textsuperscript{*}
   &
  $[0, \ensuremath{2.3\times 10^{-6}})$
\\
\hline

Immunity {\small (yr)} & $\muRS^{-1}$ & $\ensuremath{1.4\times 10^{11}}$\textsuperscript{*}
   &
  $(1410, \inf)$
\\
\hline

\end{tabular}
\begin{flushleft}
\textsuperscript{*}As evident in Figure~\ref{fig:m2Profs}, the true MLE for these parameters is $0$ and $\infty$, respectively; this value could not be obtained numerically due to the parameter transformation applied to the parameter for the model fitting processes.
\end{flushleft}
\end{table}

\subsection{Model~3 parameters}

Parameter estimates for Model~3, along with the MCAP confidence intervals for the estimate, are given in Table~\ref{tab:mod3CI}.
Figure~\ref{fig:m3Profs} displays the Monte Carlo evaluations of the profile likelihood values, obtained using a particle filter.
The total computational burden of this profile likelihood search was 28938 hours, which was computed in parallel using 7568 separate jobs via the \texttt{batchtools} R package \cite{batchtools}.





\begin{figure}[ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/m3Profs-1} 
\end{knitrout}
\caption[MCAP confidence intervals for Model 3 parameters.]{\label{fig:m3Profs}MCAP confidence intervals for Model 3 parameters. The vertical blue line indicates the smoothed MLE.}
\end{figure}

\begin{table}[!h]
\centering
\caption[Model~3 parameter estimates and confidence intervals.]{\label{tab:mod3CI}Model~3 parameter estimates and their corresponding confidence intervals, obtained via the MCAP algorithm.}
\vspace{2mm}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{Mechanism} & \textbf{Parameter} & \textbf{MLE} & $\bm{95\%}$ \textbf{Confidence Interval}
\\
\hline
\hline

 Process Noise {\small (wk\textsuperscript{1/2})} & $\sigmaProc$ & $0.218$
   &
  $(0.203, 0.230)$
\\
\hline

 Water Survival {\small (wk)} & $\Wremoval^{-1}$ & $0.108$
   &
  $(0.087, 0.110)$
\\
\hline

 Human to Water Shedding {\small $\frac{\mathrm{km^2}}{\mathrm{wk}}$} & $\Wshed$ & $\ensuremath{9.77\times 10^{-7}}$
   &
  $(\ensuremath{8.64\times 10^{-7}}, \ensuremath{1.25\times 10^{-6}})$
\\
\hline

 Asymptomatic Shedding & $\asymptomRelativeShed$ & $0.008$
   &
  $(0.0, 0.095)$
\\
\hline

 Seasonality & $\seasAmplitude$ & $1.000$
   &
  $(0.637, 1.432)$
\\
\hline

 Seasonality & $\rainfallExponent$ & $0.780$
   &
  $(0.498, 1.041)$
\\
\hline

 Reporting Rate & $\reportRate$ & $0.983$
   &
  $(0.789, 1.000)$
\\
\hline

 Observation Variance & $\obsOverdispersion$ & $88.578$
   &
  $(66.034, 132.563)$
\\
\hline

\end{tabular}
\end{table}

\section{Replication of Lee et al.~(2020) results}\label{sec:appendix_haiti_lee20}

In this article we claimed that we were able to obtain better fits to the observed data using the same models that were proposed by \citet{lee20}.
Along with visual comparisons to the data, this claim was supported by comparing likelihoods and AIC values in Table~2 in the manuscript.
Because model likelihoods were not provided by \citet{lee20}, it is necessary to replicate these models in order to obtain likelihood estimates.
Here we would like to thank the authors of \citet{lee20}, who provided detailed descriptions of their models, which enabled us to build on their work.
In the following subsections, we use our \code{R} package \code{haitipkg} to reproduce some of the results of \citet{lee20}.
This reproduction allows us to estimate the likelihoods of the \citet{lee20} version of Models~1--3, and also provides a demonstration of the importance and usefulness of reproducible research.

\subsection{Model~1 Replication}

The model was implemented by a team at Johns Hopkins Bloomberg School of Public Health (hereafter referred to as the Model~1 authors) in the \code{R} programming language using the \code{pomp} package \citep{king16}.
Original source code is publicly available with DOI: 10.5281/zenodo.3360991.
The final results reported by the Model~1 authors were obtained by using several different parameter sets rather than a single point estimate.
According to the supplement materials, this was because model realizations from a single parameter set retained substantial variability, but multiple realizations from a collection of parameter sets resulted in a reasonable visual fit to the data.
We are also inclined to believe that the use of multiple parameter values was in part intended to account for parameter uncertainty---the importance of which was discussed in the main text---an effort by the Model~1 authors that we applaud.
Simulations from each of the parameter sets, however, were treated with equal importance when being used to diagnose the model fit and make inference on the system.
This is problematic given Figures S8 and S9 of the supplement material, which suggest that some parameter sets that were used for inference may have been several hundred units of log-likelihood lower than other parameter sets that were simultaneously used to make forecasts.
Such a large difference in log-likelihoods is well beyond the threshold of statistical uncertainty determined by Wilks' theorem, resulting in the equal use of statistically inferior parameter sets in order to make forecasts and conduct inference on the system.

To fully reproduce the results of the Model~1 authors, it is necessary to use the exact same set of model parameters that were originally used to obtain the results presented by \citet{lee20}.
Because these parameter sets were not made publicly available, we relied on the source code provided by the Model~1 authors to approximately recreate the parameter set.
Due to software updates since the publication of the source code, we were unable to produce the exact same set of parameters.
Running the publicly available source code, however, resulted in a set of parameters that are visually similar to those used by the Model~1 authors (See Figures~\ref{fig:PlotEpiDist} and \ref{fig:plotEndParams}).
Furthermore, simulations using the set of parameters produced by the source code appear practically equivalent to those displayed by \citet{lee20} (See Figure~\ref{fig:plotMod1Sims}).



\begin{figure}[!h]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/PlotEpiDist-1} 

}


\end{knitrout}
\caption[Replicating Model 1 epidemic phase parameter distributions]{\label{fig:PlotEpiDist}
Bivariate distributions of parameter estimates after fitting epidemic phase of the Model~1 following the procedure described by \citet{lee20}. Compare to Figure S8 in the supplement of \citet{lee20}.
}
\end{figure}





\begin{figure}[!h]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/plotEndParams-1} 

}


\end{knitrout}
\caption[Replicating Model 1 endemic phase parameter distributions]{\label{fig:plotEndParams}
Bivariate distributions of parameter estimates after fitting endemic phase of the Model~1 following the procedure described by \citet{lee20}. Compare to Figure~S9 in the supplement of \citet{lee20}.
}
\end{figure}



\begin{figure}[!h]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/plotMod1Sims-1} 
\end{knitrout}
\caption[Simulations from replicated Model 1]{\label{fig:plotMod1Sims}
Simulations from Model~1 using parameter sets that were generated by running source code provided by \citet{lee20}. Compare to Figure~S7 in the supplement of \citet{lee20}. The upper bound for the likelihood of this model is -3031.
}
\end{figure}

Because the model forecasts provided by \citet{lee20} come from various sets of parameters---which each correspond to a unique log-likelihood value---it is not obvious how one would obtain an estimate for the log-likelihood of the model that was used for simulations by the Model~1 authors.
One approach could be to calculate the logarithm of the weighted mean of the likelihoods for each parameter sets used to obtain the forecasts, where the weights are proportional to the number of times the parameter set was used.
However, in an effort to not underestimate the likelihood of the model of the Model~1 authors, we report the estimated log-likelihood as the log-likelihood value corresponding to the parameter set with the largest likelihood value, even though the majority of simulations were obtained using parameter sets with lower likelihood values.
In this sense, we consider the log-likelihood reported in Table~1 of the main text to be an upper-bound of the log-likelihood of the model used by \citet{lee20}.
For each parameter set, the log-likelihood was estimated using a particle filter, implemented as the \texttt{pfilter} function in the \texttt{pomp} package.

\subsection{Model~2 Replication}\label{sec:mod2rep}

Model~2 was developed by a team that consisted of members from the Fred Hutchinson Cancer Research Center and the University of Florida (hereafter referred to as the Model~2 authors).
While Model~2 is the only deterministic model we considered in our analysis, it contains perhaps the most complex descriptions of cholera in Haiti: Model~2 accounts for movement between spatial units; human-to-human and environment-to-human cholera infections; and transfer of water between spatial units based on elevation charts and river flows.

The source code that the Model~2 authors used to generate their results was written in the \code{Python} programming language, and is publicly available at \url{10.5281/zenodo.3360857} and its accompanying GitHub repository \url{https://github.com/lulelita/HaitiCholeraMultiModelingProject}.
In order to perform our analysis in a unified framework, we re-implemented this model in the \code{R} programming language using the \code{spatPomp} package \citep{asfaw24}, which facilitates the creation of meta-population models.
We note that the travel and water matrices used to implement the complex dynamics in Model~2 are not available in either the Zenodo archive or the GitHub repository;
instead, we obtained these matrices via personal correspondence with the Model~2 authors.
Using these matrices, and the point estimates for model parameters provided by \citet{lee20}, we created trajectories of the cholera dynamics and compared this to available data.
These trajectories, shown in Figure~\ref{fig:mod2rep}, are very similar to the trajectories shown in Figure~S15 of the supplement of \citet{lee20}.




\begin{figure}[!h]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/plotModel2Rep-1} 
\end{knitrout}
\caption[Model~2 trajectories.]{\label{fig:mod2rep}
Model 2 trajectories using the \code{haitipkg}. Compare to Figure S15 in the supplement of \citet{lee20}.
}
\end{figure}

There are minor differences between Figure~\ref{fig:mod2rep} and Figure~S15 of \citet{lee20}.
While the discrepancy appears minor, the deterministic nature of Model~2 implies that an exact replication of model trajectories should be possible.
In this case, these discrepancies may possibly be attributed to implementing the model and plotting the model trajectory in two different programming languages.
Another potential explanation for the discrepancy is that the parameters that we used are only approximately the same as those used by \citet{lee20}.
For example, the parameters $\transmission$, $\Wbeta{}$ had reported values of $9.9 \times 10^{-7}$ and $4.03 \times 10^{-2}$, respectively (Table~S13 of the supplement material of \citet{lee20}), but were actually fit to data and therefore likely these values have been rounded.
Additionally, our implementation of Model~2 used a time scale of years and many of the parameters were reported on a weekly scale, so small differences may result due to unit conversions.
The collective effect of these small differences in model parameters likely will result in small differences in model trajectories.

Some additional concerns about being able to accurately replicate the results of \citet{lee20} are valid.
Details about the measurement models and how latent states were initialized for the epidemic model were not provided by \citet{lee20} and therefore these details must be inferred by looking at the provided source code.
According to repository comments, the files \code{fit\allowbreak In\allowbreak Pieces\allowbreak 3params\allowbreak Clean\allowbreak May2019\allowbreak Public.py} and \code{fit\allowbreak In\allowbreak Pieces\allowbreak Mu\allowbreak With\allowbreak Frac\allowbreak Sus\allowbreak Fixed\allowbreak All\allowbreak Infections\allowbreak Public.py} were used to fit the epidemic and endemic phases of the model respectively, although it is apparent that these exact files were not used to obtain the reported results since the files contain some variable-naming errors that make it impossible to run the files without making modifications \footnote{One example of why the code cannot be run that the file loads functions from a non-extant file named \code{choleraEqs.py} in line 13 rather than \code{cholera\allowbreak Eqs\allowbreak Public.py}.}.
The inability to replicate the results by \citet{lee20} by running the provided source code makes checking whether or not a our numeric implementation faithfully represents their results very difficult.
Additionally, the script that was said to been used to obtain the results reported by \citet{lee20} appears to use a different measurement model than what was described in the supplemental material, again making it difficult to fully replicate the result of \citet{lee20} without being able to easily run the provided source code.
In this case, we chose to use measurement model that considers only symptomatic individuals for both phases of the epidemic, as this seemed to visually match the results of \citet{lee20} most closely.

\subsection{Model~3 Replication}

Model~3 was developed by a team of researchers at the Laboratory of the Swiss Federal Institute of Technology in Lausanne, hereafter referred to as the Model~3 authors.
The code that was originally used to implement Model~3 is archived with the DOI: \url{10.5281/zenodo.3360723}, and also available in the public GitHub repository: \code{jcblemai/haiti-mass-ocv-campaign}.
Because the code was made publicly available, and final model parameters were reported in the supplementary material of \citet{lee20}, we were able to reproduce Model~3 by directly using the source code.
In Figure~\ref{fig:mod3rep}, we plot simulations from this model.
This figure can be compared to Figure S18 of \citet{lee20}.
We note that slight differences may be accounted for due to variance in the model simulations and the difference in programming language used to produce the figure.
Overall, the high standard of reproducibility that was achieved by the Model~3 authors facilitated the ability to readily replicate their model and results.



\begin{figure}[!h]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/PlotMod3Rep-1} 
\end{knitrout}
\caption[Simulations from replicated Model 3]{\label{fig:mod3rep}
Simulations from Model~3.
Compare to Figure S18 in the supplement of \citet{lee20}.
}
\end{figure}



\section{Calibrating Model~3 to observed cases}\label{sec:appendix_haiti_mod3Cal}












In this section, we provide more detail on the process that was used to estimate the coefficients of Model~3.
In particular, we discuss why we decided to include additional model parameters---those that are associated with the behavior of the system during Hurricane Matthew---that were not considered by \cite{lee20}.
To calibrate this model, we used the iterated block particle filter (IBPF) method of \cite{ionides24}.
Due to the novelty of this algorithm, there exists only a few examples where the IBPF algorithm is used for data analysis \cite{li23,ionides24}, which is one motivation of the inclusion additional details related to fitting and diagnosing the model fit provided here.

Lee et al. (2020)~\cite{lee20} only estimated model parameters to a simplified version of Model~3 on a subset of the available data, as no method existed at the time of their publication to fit a fully coupled meta-population model to disease incidence data.
Building on their results, we fit the fully coupled version of Model~3 to (nearly) all available data, reserving only a few observations to use to calibrate the initial conditions of the model (see the supplement for initialization models for more details).
To maximize model likelihoods, we relied on parameter estimates obtained while calculating profile-likelihood confidence intervals, as this calculation requires many replicated IBPF searches.
In our preliminary investigations that were done prior to conducting a profile likelihood search, we found that it was necessary to use multiple searches for the MLE, periodically pruning away less successful searches.
To do this, the first collection of searches is performed by obtaining initial values for the parameters by uniformly sampling values from a predefined hypercube.
A subsequent refinement search used parameter values corresponding to the largest model likelihoods as starting parameter values.
The need for multiple searches does not appear to be uncommon, as a similar approach was used in \cite{ionides24}.
While computationally intensive, profile likelihoods proved to be an effective alternative to maximizing model likelihoods without the need to apply this multistage heuristic.

We use the iterative fitting / pruning technique described above to fit the fully coupled version of Model~3 proposed in \cite{lee20}.
The maximum likelihood we obtained after two rounds of searching was $\ensuremath{-1.7549\times 10^{4}}$, which is higher than the benchmark model ($\ensuremath{-1.7933\times 10^{4}}$).
While beating a simple associative benchmark is promising, this does not immediately imply that the model is a good description of the system.
Additional investigation of parameters estimates and their corresponding implications on model based conclusions should always be conducted.
For meta-population models, it is worth considering how well the model fits the data to each spatial unit.
% This can be done by looking at conditional log-likelihoods, which is part of the output of the \texttt{bpfilter} algorithm in the \texttt{spatPomp} package.
The likelihoods for each department, compared to the corresponding benchmark model, are displayed in Figure~\ref{fig:h3UnitLikes}.
The figure demonstrates that while the log-likelihood of the fitted model outperforms the auto-regressive negative binomial benchmark model at the aggregate level, Model~3 has lower likelihoods for some departments.

\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/h3UnitLikes-1} 
\end{knitrout}
\caption[Model~3 unit log-likelihoods.]{\label{fig:h3UnitLikes}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model prior to the inclusion of parameters that account for Hurricane Matthew.}
\end{figure}

In addition to considering the conditional log-likelihoods of each unit, one can consider conditional log-likelihoods of each observation.
When compared to a benchmark, this level of detail can provide useful information about which observations are well described by the model and which are not.
In Figure~\ref{fig:condLL}, we plot the conditional log-likelihoods of Model~3 for each observation.
Typically it is most useful to compare the conditional log-likelihoods of the model under consideration to a benchmark, as plotting only conditional log-likelihoods without a comparison may not be helpful.
In this case, however, the same insight can be drawn using a figure without a benchmark comparison, so we do not include the benchmark in order to avoid the issue of over-plotting.

\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/CondLLFig-1} 
\end{knitrout}
\caption[Conditional log-likelihoods without hurricane adjustment]{\label{fig:condLL}Conditional log-likelihoods of Model~3 prior to the inclusion of the Hurricane Matthew related parameters.}
\end{figure}

Figure~\ref{fig:condLL} reveals that the fitted model poorly describes certain features of the data.
For example, many departments (in particular Sud) have observations with lower conditional log-likelihoods near October 2016 than at other time points.
Further investigation of the model output reveals that the model is struggling to explain the sudden surge in cholera cases that occurred at this time, which coincides with the time that Hurricane Matthew struck Haiti.
While the model does include a mechanism to account for increased cholera transmission due to large rainfall events, the mechanism does not appear to be sufficient to capture the damaging effects of the hurricane, which had the greatest impact in the the Sud and Grand'Anse departments \cite{ferreirai16}.
This result led us to include parameters $\Whur{u}$ and $\hHur{u}$ in Eq.~23 of the main text, which allows for an increase in the transmission rate between environmental cholera and humans for in Sud and Grand'Anse during and after the hurricane.
The effect of the hurricane on cholera transmission is assumed to have an exponential decay, where the magnitude is determined by $\Whur{u}$ and the duration of the effect determined by $\hHur{u}$.



We refit Model~3 after introducing these hurricane-related parameters.
The resulting model has a log-likelihood value of $\ensuremath{-1.73329\times 10^{4}}$.
The inclusion of these parameters resulted in an overall increase of $216.4$ log-likelihood units.
Such a large difference in log-likelihoods is well beyond the threshold of statistical uncertainty determined by Wilks' theorem, suggesting that the data highly favor the inclusion of the additional parameters.
The addition of the Hurricane parameters also increases in conditional likelihoods for each observation, particularly around October 2016 (Figure~\ref{fig:finalCondLL}).



\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/finalCondLLFig-1} 
\end{knitrout}
\caption[Conditional log-likelihoods with hurricane adjustment]{\label{fig:finalCondLL}Conditional log-likelihoods of Model~3 after adding and estimating the parameters related to Hurricane Matthew.}
\end{figure}

Now that the model with additional parameters has been calibrated to the incidence data, we plot the conditional log-likelihood of each department compared to a benchmark model in Figure~\ref{fig:finalUnitLL}.
The difference in log-likelihoods between Model~3 and the benchmark model is smallest in the departments Artibonite, Nord, Ouest and Centre.
Each of these departments also exhibited the most sustained cholera transmission, defined by having the fewest number of weeks with no recorded cholera cases.
Specifically, these four departments have zero cholera cases recorded in less than $4\%$ of the available data, and all remaining departments---except for Nord-Ouest, which has approximately $9.5\%$ of cases that are zeros and also exhibits the next smallest difference in log-likelihoods---have zero cases recorded in at least $14\%$ of the available weekly data.
This result suggests that the quantitative advantage Model~3 has over its respective benchmark is primarily due to the model's ability to describe a resurgence of cases after a department records a week of zero cholera cases.
This result may be unsurprising in the context of the models that we are comparing.
Because the cholera transmission in individual departments likely depends on the national prevalence of cholera and the Vibrio cholerae bacteria, our spatially-independent benchmark model that relies exclusively on the previous number of case within any given unit has a difficult time predicting a resurgence of cases.

\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/finalUnitLLFig-1} 
\end{knitrout}
\caption{\label{fig:finalUnitLL}Log-likelihoods of Model~3 for each department compared to the corresponding benchmark model after adding and estimating parameters related to Hurricane Matthew.}
\end{figure}

The difference in log-likelihoods between Model~3 and its benchmark model for each individual units suggests that Model~3 has a relatively poor fit for the four units with the most sustained cholera transmission.
The simple four parameter benchmark has a higher likelihood than Model~3 for the Artibonite and Nord departments, and also has log-likelihoods only a few units smaller than Model~3 for the departments Ouest and Centre.
This is particularly worrisome given that these four departments account for more than $77\%$ of the total number of reported cholera cases.

\subsection{Examining the Hidden States of the Calibrated Model}

For mechanistic models, beating a suitable statistical benchmark does not alone guarantee that the model provides an accurate description of a dynamic process.
Indeed, a good statistical fit does not require the model to assert a causal explanation.
For example, reconstructed latent variables should make sense in the context of alternative measurements of these variables \cite{grad12}.
We demonstrate this principle by examining the latent state of the calibrated model.
In particular, we examine the compartment of susceptible individuals under various scenarios.
This analysis can also provide insight into why the calibrated model fails to outperform the benchmark model on the four departments with the most sustained cholera transmission.

Recall that the filtering distribution for the calibrated version of Model~3 at time $t_k$ is defined as the distribution of the latent state at time $t_k$ given the data from times $t_{1}:t_{k}$, i.e. $f^{(3)}_{\bm{X}_k|\bm{Y}_{1:k}}(\bm{x}_{k} | \bm{y}^*_{1:k} ; \hat\theta)$.
In general, one may expect simulations from the filtering distribution of a model with a good statistical fit to result in hidden states that are highly consistent with the observed data because the filtering distribution is conditioned on the observed data.
Figure~\ref{fig:h3Sus} shows the percentage of individuals that are in the susceptible compartment from various simulations of the model:
simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
This figure shows that simulations from initial conditions tends to result in a much more rapid depletion of susceptible individuals at the start of the epidemic than simulations from the filtering distribution, suggesting the calibrated model has a propensity to predict larger outbreaks than what is typically seen in the data.
This result demonstrates that the calibrated model favors a more rapid growth in cholera cases than what is typically seen in the observed data, providing a possible explanation as to why the model fails to beat the simple benchmark for each spatial unit.
This results hints at the possibility of model mispecification, and warrants a degree of caution in interpreting the model's outputs.



\begin{figure}[!ht]
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}
\includegraphics[width=\maxwidth]{figure/h3SusFig-1} 
\end{knitrout}
\caption[Estimated susceptible population over time.]{\label{fig:h3Sus}Percentage of individuals that are in the susceptible compartment.
Simulations from Model~3 under initial conditions are displayed in red; simulations from the filtering distribution of model are displayed in blue.
The dashed line represents the end of the observed data.}
\end{figure}

\section{Forecasting with parameter uncertainty}\label{sec:appendix_haiti_uncertain}

Let $f_{Y_{1:N}}(y_{1:N} | \theta)$ denote the pdf of the model under consideration, were $\theta$ is a parameter vector that indexes the model.
Furthermore, denote the observed data as $y_{1:N}^*$.
Because the uncertainty in just a single parameter can lead to drastically different forecasts \cite{saltelli20},
parameter uncertainty should be considered when obtaining model forecasts when the goal is to influencing policy.
In a Bayesian modelling paradigm, the most natural way to account for parameter uncertainty in model forecasts is to suppose that $\theta$ comes from a distribution $f_{\Theta}$, and then to obtain $J$ forecasts from the model where each forecast is obtained using parameters drawn from the posterior distribution $\theta_{1:J} \mid Y_{1:N} = y_{1:N}^* \sim f_{\Theta}\big(\theta | Y_{1:N} = y_{1:N}^*\big)$.

When frequentist methods are used, however, there does not exist a posterior distribution from which one could sample.
A common approach could be to obtain a weighted average of the simulations from various models \cite{hoeting99}, but this can be problematic when forecasts from each model are very different from each other \cite{grueber11}.
A similar approach that has been taken \cite{king15} is to obtain model forecasts using multiple sets of parameter values and then sample from the resulting forecasts using weights proportional to the corresponding likelihoods of the parameter values.
This approach could be considered as empirical Bayes, as it is equivalent to using a discrete uniform prior where the set of values in the prior distribution is determined by a stochastic routine applied to the observed data, as discussed below.

% Let $\Theta$ denote a random vector of model parameters.
For each $k \in 1:K$, let $\theta_k$ be a unique set of model parameters.
Letting $\Theta$ denote a random vector of model parameters, we endow $\Theta$ with a discrete uniform distribution on the set $\{\theta_1, \theta_2, \ldots, \theta_K\}$, such that $P\big(\Theta = \theta_k\big) = \frac{1}{K}$ for all values $k \in \seq{1}{K}$.
Using this as a prior distribution, the posterior distribution of $\Theta | Y_{1:N} = y_{1:N}^*$ can be expressed as:
$P\big(\Theta = \theta_k | Y_{1:N} = y_{1:N}^*\big) = \frac{f_{Y_{1:N}}(y_{1:N}^*| \theta_k)}{\sum_{l = 1}^K f_{Y_{1:N}}(y_{1:N}^*| \theta_l)}$.
In a standard empirical Bayes analysis, the values $\theta_1, \ldots, \theta_k$ of the prior distribution would be chosen using the observed data, resulting in a posterior distribution that weighs the prior parameter vectors proportional to their corresponding likelihoods.
We choose $\theta_k$ to be the output of a stochastic routine applied to the observed data by setting $\theta_k$ to be the output of an iterated filtering algorithm.
In practice, because the likelihood maximization routines of iterated filtering methods are stochastic, it is common to run the iterated filtering method multiple times $(K)$ for each model in order to obtain a maximum likelihood estimate for model parameters.
This results in a natural set of parameters near the MLE that could be used as the discrete prior distribution.
Alternatively, the set $\{\theta_1, \theta_2, \ldots, \theta_K\}$ could be determined by first obtaining marginal confidence intervals for each element of the parameter vector $\Theta$, and then creating a hypercube using the combination of marginal confidence intervals.
The set $\{\theta_1, \theta_2, \ldots, \theta_K\}$ is then obtained by sampling uniformly $K$ values from the resulting hypercube, as was done by \cite{king15}.


%%%%%%%%%% END
