 \section{Proof and discussion of Theorem~1}\label{sec:panelTheory}
 
  Theorem~\ref{theorem:pif} is a straightforward extension of Theorem~4 of \citet{chen24} to PanelPOMP models. 
  Our approach is to express an arbitrary PanelPOMP model as a POMP model using a long format, following the nomenclature from the R \code{tidyverse} \citep{wickham19}.
  Then, Algorithm~\ref{alg:mpif} is equivalent to applying Algorithm~4 of \citet{chen24} to this long POMP model.
  After ensuring the necessary conditions are met, their Theorem~4 provides guarantees for this to converge to the MLE, proving the conclusions of Theorem~\ref{theorem:pif}. 
  
  This stacking argument follows the approach of \citet{breto20}, who previously introduced the PIF algorithm and extended an existing theory for low-dimensional POMP models \citep{ionides15} to derive theoretical properties of PIF. 
  However, the recent work of \citet{chen24} provides convergence results for iterated filtering algorithms under weaker assumptions than \citet{ionides15}.
  Most notably, \citet{chen24} prove convergence of iterated filtering algorithms as the random walk standard deviation for parameter perturbations decreases over time rather than being fixed at a small constant.
  Thus, this approach allows us to provide stronger theoretical results for panel iterated filtering than obtained by \citet{breto20}.
  
  \begin{proof} 
  % Let $\Theta_j^{(M)}$ denote the output of $M$ iterations of Algorithm~\ref{alg:mpif} without marginalization, and 
  Recall the basic definition of the joint density of a PanelPOMP model: 
  \begin{align}
&f_{\bm{X}_{0:N}, \bm{Y}_{1:N}}(\bm{x}_{0:N}, \bm{y}_{1:N}; \, \theta) =
\nonumber
\\
& \hspace{20mm} \prod_{u = 1}^Uf_{X_{u, 0}}(x_{u, 0};\, \theta) 
\prod_{n = 1}^{N_u}f_{Y_{u, n}|X_{u, n}}(y_{u, n} | x_{u, n} ;\, \theta)f_{X_{u, n} | X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \, \theta). \label{eq:ppompSI}
\end{align}
  We would like to pivot the unit specific processes into a single long process, avoiding the need to loop over the unit $u$.
  We define $S_{\tilde{u}} = \sum_{k = 1}^{\tilde{u}}(N_k + 1)$ to be the sum total number of time-steps (observations + initialization) for units $1$ up to unit $\tilde{u}$, defining $S_0 = 0$.
  We use the sequence $\nstack \in \mathbb{N}$ is used to map time points and states indexed with $(u, n) \in \seq{1}{U}\times\seq{0}{N_u}$ to a new model with only a single index $\nstack \in \seq{1}{S_U}$, defined by the equation $\nstack = S_{\unit-1} + n + 1$
  % For convenience, we refactor the model to initialize at time $t_{u, 1}$ rather than $t_{u, 0}$ for each unit $u$ by considering the density of $X_{u, 1}$ after integrating out $X_{u, 0}$. 
  % That is, we use the identity 
  % $$
  % f_{X_{u,1}}(x_{u, 1};\, \theta) = \int_{\mathcal{X}} f_{X_{u, 1}|X_{u, 0}}(x_1|x_0;\, \theta)f_{X_{u, 0}}(x_0;\, \theta)dx_0
  % $$
  % and use this to write Eq.~\ref{eq:ppompSI} as
  % \begin{align}
  % f_{\bm{X}_{1:N}, \bm{Y}_{1:N}}(\bm{x}_{1:N}, \bm{y}_{1:N}; \, \theta) = \prod_{u = 1}^U \prod_{n = 1}^{N_u}f_{Y_{u, n}|X_{u, n}}(y_{u, n} | x_{u, n} ;\, \theta)f_{X_{u, n} | X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \, \theta), \label{eq:ppompSI1}
  % \end{align}
  % using the convention that for $n = 1$, $f_{X_{u, n} | X_{u, n-1}}(x_{u, n}|x_{u, n-1}; \, \theta) = f_{X_{u,1}}(x_{u, 1};\, \theta)$.
  
  Now let $T_{\tilde{u}} = \sum_{k = 1}^{\tilde{u}} (t_{k, N_k} - t_{k, 0})$ for $\tilde{u} \in \seq{1}{U}$, with $T_0 = 0$.
  We now define new latent processes $\tilde{X}$ and $\tilde{Y}$. As the original model allows for continuous time latent process, we write 
  First,
  \begin{align}
    \tilde{X}(t) &= X_u(t_{u, 0} + (t - T_u)), \, \text{for} \,\, T_{u-1} \leq t \leq T_u.
  \end{align}
  Letting $\tau_{\nstack} = t_{u, n}$ using the mapping $\nstack = S_{\unit-1} + n + 1$, the latent and observable processes are equivalent to
  \begin{align*}
  \begin{split}
    \tilde{X}_{\nstack} = \tilde{X}(\tau_{\nstack}) = X_{u, n} \\
    \tilde{Y}_{\nstack} = Y_{u, n}. 
  \end{split}
  \end{align*}
  With this new definition of $\tilde{X}$ and $\tilde{Y}$, we have a new POMP model which has joint density
  \begin{align}
&f_{\tilde{X}_{1:S_U}, \tilde{Y}_{1:S_U}}(x_{1:S_U}, y_{1:S_U}; \, \theta) =  \nonumber
\\
& \hspace{20mm}
f_{\tilde{X}_0}(x_0;\, \theta)\prod_{\nstack = 1}^{S_{U}}f_{\tilde{Y}_{\nstack}|\tilde{X}_\nstack}(y_{\nstack} | x_{\nstack}; \, \theta)f_{\tilde{X}_{\nstack}|\tilde{X}_{\nstack - 1}}(x_{\nstack} | x_{\nstack - 1}; \, \theta), \label{eq:lowPOMP}
  \end{align}
  Using the convention that $n' \mapsto (u,0)$ for any $u$, then $f_{\tilde{Y}_{\nstack}|\tilde{X}_\nstack}(y_{\nstack} | x_{\nstack}; \, \theta) = 1$ and \linebreak $f_{\tilde{X}_{\nstack}|\tilde{X}_{\nstack - 1}}(x_{\nstack} | x_{\nstack - 1}; \, \theta) = f_{X_{u, 0}}(x_{u, 0}; \, \theta)$.

  As defined, Eqs.~\ref{eq:lowPOMP} and \ref{eq:ppompSI} describe the same model, but \ref{eq:lowPOMP} has been written to match the state space model of \citet{chen24}, after adjusting for the choice of initializing at $\tilde{X}_1$ rather than $\tilde{X}_0$. 
  We refer to Eq~\ref{eq:lowPOMP} as the \emph{long} format, which is indicative that the model has been expressed as a low-dimensional POMP model with observation times ranging from $1$ to $S_U$, rather than a collection (or product) of POMP models, each with observation times $N_u+1$ for $u \in \seq{1}{U}$. 
  
  This representation allows us to naturally extend the theoretical framework of \citet{chen24} to PanelPOMP models.
  Specifically, Assumption~\ref{assumption:regular} imply that the set $\Theta$ is a regular compact set \citep[See Definition~1 of][]{chen24}; Assumptions~\ref{assumption:mle1}--\ref{assumption:mle2} ensure that the density in Eq.~\ref{eq:lowPOMP} corresponds to a state-space model that satisfies the MLE conditions of \citet{chen24}. 
  Finally, Assumptions~\ref{assumption:kernel1}--\ref{assumption:kernel4} are applied to the cloned version of Model~\ref{eq:lowPOMP}. Specifically, the process is cloned such that we have a new state space model $\hat{Y}_{\nclone} = Y_{u, n}$, $\hat{X}_{\nclone} = X_{u, n}$, using the mapping $\nclone = (\nmif-1)S_\Unit + S_{\unit-1}+n+1$.
  Assumptions~\ref{assumption:kernel1}--\ref{assumption:kernel4} therefore imply assumptions C1-C3 and C4' of \citet{chen24} for the perturbation kernels of the self organized state space model defined via $\big(\hat{Y}_{\nclone}, \hat{X}_{\nclone}\big)$.
  Together, the conditions stated in Appendix~\ref{sec:assumptions} allow for a direct application of Theorem~4 of \citet{chen24} to this model.
  % See S6.2.3 and S6.2.4 of Chen et al (2024). Here, all numbers 
  % are referencing the Chen et al (2024) paper unless otherwise stated. To use 
  % Theorem~4, which gives convergence of the particle representation, we need 
  % Propositions S10-S11, and Theorems A.1 / A.2: 
  % 
  %    - Theorems A.1 and A.2 give same results, with different conditions. 
  %      Specifically, A.2 assumes conditions C1-C3 + C4'. In our version of 
  %      the assumptions, we only give C4' (which is our assumption (C4)), so 
  %      this only gives us A.2, which is all that is needed. To further satisfy 
  %      A.2, we need: 
  %       - We need assumptions A1-A2 to hold. Assumption MLE implies A1 (by 
  %         proposition S10). We satisfy assumption MLE with our assumptions 
  %         B1-B2. 
  %       - They never say the explicitly, but A2 is satisfied by the MLE 
  %         condition. Specifically, G_{s, \theta}(x', x) = f(y|x; \theta) 
  %         (from Proposition S9), and Assumption MLE says that 
  %         sup f(y|x; \theta) is finite.
  %    - Proposition S10 is satisfied by Assumption (MLE), and the definition 
  %      of the model (Proposition S9). 
  %    - Proposition S11 is readily satisfied by the model and perturbation 
  %      definitions (Proposition S9). 
  % 
  % Our additional assumption (A1) we haven't mentioned yet is just needed to 
  % ensure the regular compact set (Definition 1 of Chen et al (2024)).
  % 
  % Finally, the assumptions C1-C3 + C4' are satisfied for Gaussian 
  % perturbations via Section S3, prop S2 (also see comment about pomp package, 
  % which gives the PIF defaults, in Section 3.3). 
    \end{proof}

  \noindent Formally, Theorem~\ref{theorem:pif} provides guarantees for several variants of iterated filtering algorithms applied to panel models, where each variant is a change to the perturbation kernel. 
  However, not all variants are useful in practice. 
    For instance, directly applying IF2 to panel models generally leads to worse results than the PIF algorithm applied to the same model \citep{breto20}.
  
  As pointed out in Section~\ref{sec:discussion}, the effect of the perturbation kernel is twofold. 
    First, it helps revive particle representations of the intermediate parameter distributions by adding random noise. 
    Second, the random noise results in a loss of information by masking the signal from the observed data. 
    However, these competing interests are not addressed in theorems involving iterated filtering algorithms.
    In this case, heuristics are useful for determining suitable perturbation densities. 
    
    For instance, it can be shown following Proposition~S2 of \citet{chen24} that choosing the perturbation density $h_{u, n}(\theta|\varphi; \sigma_{u, m})$ in lines~\ref{line:startu} and \ref{line:perturbations} of Algorithm~\ref{alg:mpif} to be a multivariate normal density satisfies Assumptions~\ref{assumption:kernel1}--\ref{assumption:kernel4}, where $\varphi$ is the mean of the distribution and, for some full rank positive definite matrix $\Sigma_0$ and sequence $\sigma_{u, m} = o(m^{-1})$, $\sigma^2_{u, m}\Sigma_0$ is the covariance.
    
    If $\Sigma_0$ is full rank, however, then the perturbations will be applied to all parameters at each time step, including unit specific parameters for units $v \in \{1, \ldots, U\} / \{u\}$. 
    This results in an unnecessary amount of loss of information at each step.
    This motivates the default behavior of the PIF algorithm, which is that each unit gets an initial covariance $\Sigma_{u, 0}$, and only the columns corresponding to the shared and unit-specific parameters of interest are full rank. 
    Explicitly, if we write $\Sigma_{u, 0}^{(\phi, \psi_u)}$ as the full-rank submatrix corresponding the relevant shared and unit specific parameters, then the perturbation in line~\ref{line:perturbations} in Algorithm~\ref{alg:mpif} is equivalent to 
\begin{equation*}
  \begin{cases}
    \big(\Phi_{n}, \Psi_{u, n} \big) \sim N\Big[\big(\Phi_{n-1}, \Psi_{u, n-1} \big), \, 
        \sigma^2_{u, m}\Sigma_{u, 0}^{(\phi, \psi_u)}\Big]
    \\
    \Psi_{v, n} = \Psi_{v, n-1} \text{ for all} \, v \in \seq{1}{U}, \, v \neq u.
  \end{cases}  
\end{equation*}
 This strategy is effective in reducing the loss of information that results from perturbing parameters when the data provides limited information. 
 However, it also results in the particle depletion issue in higher dimensions that is discussed in detail in Section~\ref{sec:depletion}. 
 \citet{chen24} propose alternative perturbation strategies that are intended to balance these competing interests in low dimensional models. 
 In the PanelPOMP models we are interested in this study, the proposed alternatives would still require a large number of perturbations of parameters that are not of direct interest, resulting in a large loss of information. 
 For this reason, the marginalization procedure is useful in practice as it directly addresses both issues simultaneously in PanelPOMP models. 
 Combining the novel perturbations strategies of \citet{chen24} with a marginalization step could also result in an improved algorithm in some cases, but that is outside of the scope of this article. 
    
A common scenario is that the entire collection of model parameters contain a subset that are only relevant to the model dynamics at certain time steps. 
The most common instance of this is initial value parameters, which only impact how the latent dynamic process is initialized, and as such the data points that are generally most relevant for inference are at the start of the time series. 
A standard strategy for these parameters in low-dimensional settings is to only apply perturbations at observation times at the beginning of the observed time series, following the logic above for unit-specific parameters from units $v \neq u$. 
  
One reason that Theorem~\ref{theorem:pif} cannot be used directly to infer the practicality of panel iterated filtering algorithms is because the behavior of $C_M$ as $M \rightarrow \infty$ is unknown.
Previous works on high-dimensional particle filtering---without adding perturbations or performing data cloning---suggest that the sequence $C_M$ might scale exponentially with the number of units \citep{snyder08,bengtsson08}.
While this problem has partially been avoided by writing the PanelPOMP in a long format, reducing the size of both the latent and observed spaces at each time point, the parameter space for $\Theta$ is still large.
Specifically, the total dimension of $\Theta$ is $\nshared + U\nspecific$, where $\nshared$ and $\nspecific$ are the number of shared and unit-specific parameters, respectively.

Both perturbation kernels proposed above (IF2 and PIF) therefore require high-dimensional filtering, which is known to not perform well in practice \citep{rebeschini15}.
This provides another motivation for the MPIF algorithm, as it dramatically shrinks the size of the latent sates that are being filtered at each steps. 
Thus, the MPIF algorithm has many similarities to other iterated block particle filtering algorithms that have been effective for moderately sized dynamic systems with spatial coupling \citep{ning23,ionides24}.
