\chapter{Likelihood Based Inference for ARMA Models}
\label{chpt:conclusion}


<<armaSetup, include=FALSE,echo=FALSE,results='hide'>>=
# root <- paste0("chapters/arima2/")
root <- ""

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = 'center',
  dev = 'cairo_pdf'
)

library(arima2)      # Package described in this article
library(tidyverse)   # Data wrangling and visualizations
library(foreach)     # Parallel Computing
library(doParallel)  # Parallel Computing
library(doRNG)       # To get reproducible seed for parallel computing
library(pomp)        # Used for the bake function
library(knitr)       # Necessary for Rnw file
library(latex2exp)

# Setting black and white ggplot2 theme for entire document.
theme_set(
  theme_bw() +
    theme(
      plot.title = element_text(hjust = 0.5)
    )
)

AR_labs <- function(num) {
  paste0("AR = ", num)
}

MA_labs <- function(num) {
  paste0("MA = ", num)
}
@

\AtBeginEnvironment{algorithm}{%
  \singlespacing
}

\section{Introduction}
\label{sec:intro}

Auto-regressive moving average (ARMA) models are the most well known and frequently used approach to modeling time series data.
The general ARMA model was first described by Whittle \cite{whittle51}, and popularized by Box and Jenkins \cite{box1970}.
Today, ARMA models are a fundamental component of various academic curricula, leading to their widespread use in both academia and industry.
ARMA models are as foundational to time series analysis as linear models are to regression analysis, and they are often used in conjunction for regression with ARMA errors.
A Google Scholar search for articles from 2024 onward that include the phrase ``time series" and the term ``ARMA" (or variants) yields over 18,000 results.
% SEARCH TERM: "time series" ARIMA OR ARMA OR SARIMA OR SARIMAX OR ARIMAX OR SARMA OR SARMAX
While not all these articles focus on the same models discussed here, the importance of this model class to modern science cannot be overstated.
Given the ubiquity of ARMA models, even small improvements in parameter estimation constitute a significant advancement of statistical practice.

A commonly used extension of the ARMA model is the \emph{integrated} ARMA model, which extends the class of ARMA models to include first or higher order differences.
That is, an autoregressive integrated moving average (ARIMA) model is an ARMA model fit after differencing the data in order to make the data stationary.
Additional extensions include the modeling of seasonal components (SARIMA), or the inclusion of external regressors (SARIMAX).
Our methodology can readily be extended to these model classes as well, but here we focus on ARMA modeling for simplicity.

We demonstrate that the most commonly used methodologies and software for estimating ARMA model parameters frequently yield sub-optimal estimates.
This assertion may seem surprising given the extensive application and study of ARMA models over the past five decades.
A natural question arises: if these optimization issues exist, why have they not been addressed?
There are three plausible explanations: the first is that the potential for sub-optimal results has largely gone unnoticed; the second is a satisfactory solution has not yet been discovered; and the third is a general indifference to the problem.
It is likely that a combination of these factors has deterred prior exploration of this issue.
For instance, most practitioners may be unaware of the problem, while those who have noticed it either did not prioritize it or were unable to provide a general method to resolve it.
In this article, we address all three possible explanations by demonstrating the existence of an existing shortcoming, explaining why the problem has nontrivial consequences, and proposing a readily applicable and computationally efficient solution.

Imperfect likelihood optimization has an immediate consequence of complicating standard model selection procedures.
Algorithms in widespread use lead to frequent inconsistencies in which a smaller model is found to have a higher maximized likelihood than a larger model within which it is nested.
This is mathematically impossible but occurs in practice when the likelihood is imperfectly maximized, and is commonly observed using contemporary methods for ARMA models.
Such inconsistencies are a distraction for the interpretation of results even when they do not substantially change the conclusion of the data analysis.
Removing numeric inconsistencies can, and should, increase confidence in the correctness of statistical inferences.

There are various software implementations available for the estimation of ARMA model parameters.
In this article, we focus on the standard implementations in R (\code{stats} package) and Python (\code{statsmodels} module), which we selected due to their widespread usage.
While both implementations offer multiple ways to estimate parameters, the default approach in both software packages is to perform likelihood maximization, assuming that the error terms are Gaussian.
The challenges in parameter estimation arise in this situation because there is no closed-form expression for the likelihood function, though computational algorithms do exist for maximizing ARMA likelihoods \cite{gardner1980}.

We begin by providing essential background information on the estimation of ARMA model parameters.
We then present our proposed approach for parameter estimation, which leads to parameter values with a likelihood that is never lower and sometimes higher than the standard method.
This is followed by a motivating example and a discussion of the potential implications of our proposed method.
We also discuss the construction of standard errors for our maximum likelihood estimate.
Specifically, we show that estimates of the standard error for model parameters that are default output of R and Python can be misleading, and we provide a reliable alternative.
Throughout the article, we use the \code{stats::arima} function from the R programming language as a baseline for comparison.
The same methodology for fitting parameters is used in the \code{statsmodels.tsa} module in Python, and we demonstrate that our results apply to this software as well (\nameref{S_python}).

\section{Maximum Likelihood for ARMA Models}

Following the notation of Shumway and Stoffer \cite{shumway2017}, a time series $\{x_t; t = 0, \pm1, \pm2, \ldots\}$ is said to be $\text{ARMA}(p, q)$ if it is (weakly) stationary and
\begin{equation}
  x_t = \phi_1x_{t - 1} + \cdots + \phi_px_{t - p} + w_t + \theta_1w_{t-1} + \ldots + \theta_qw_{t - q}, \label{eq:arma}
\end{equation}
with $\{w_t; t = 0, \pm1, \pm2, \ldots\}$ denoting a mean zero white noise (WN) processes with variance $\sigma_w^2 > 0$, and $\phi_p \neq 0$, $\theta_q \neq 0$.
We refer to the positive integers $p$ and $q$ of Eq.~\ref{eq:arma} as the autoregressive (AR) and moving average (MA) orders, respectively.
A non-zero intercept could also be added to Eq.~\ref{eq:arma}, but for simplicity we assume that the time series $\{x_t; t = 0, \pm1, \pm2, \ldots\}$ has zero mean.
We denote the set of all model parameters as $\allVar = \{\phi_1, \ldots, \phi_p, \theta_1, \ldots, \theta_q, \sigma_w^2\}$.
Our objective is to estimate model parameters using the observed uni-variate time series data.

Given the importance of ARMA models, numerous methods have been developed for parameter estimation.
For instance, parameters can be estimated through methods such as Bayesian inference \cite{monahan83,chib94} or neural networks \cite{chon97}, among many others.
Specialized methods include those for integer-valued data \cite{weiss24}, approaches robust to outliers \cite{chakhchoukh10}, and non-Gaussian ARMA processes \cite{lii90}.
While each methodology has its merits, we focus on the approach most widely adopted in statistical practice: likelihood maximization, under the assumption that the WN process $\{w_t\}$ is Gaussian.

A relevant method for estimating model parameters is minimization of the conditional sum-of-squares (CSS).
The CSS estimate is fast to compute, but it does not posses the statistical efficiency of the maximum likelihood estimate (MLE).
However, the CSS method plays a role in likelihood maximization, so we briefly describe it here.
By solving for the WN term $w_t$, Eq.~\ref{eq:arma} can be written as
\begin{equation}
w_t = x_t - \sum_{i = 1}^p \phi_i x_{t - i} - \sum_{j = 1}^q \theta_j w_{t-j}. \label{eq:css}
\end{equation}
A natural estimator would involve minimizing the sum of squares $\sum_{t = 1}^n w_t^2$.
However, since only $x_1, x_2, \ldots, x_n$ are observed and $w_{t}$ is recursively defined in Eq.~\ref{eq:css} using values of $x_{t-p}$ and $w_{t-q}$, directly minimizing this sum is intractable.
The CSS method addresses this issue by conditioning on the first $p$ values of the process, assuming $w_p = w_{p-1} = \ldots = w_{p+1-q} = 0$, and minimizing the conditioned sum $\sum_{t = p+1}^n w_t^2$.
While the CSS method provides an attractive solution do to its relative simplicity and easiness to compute, it ignores the error terms for the first few observations.
This is particularly concerning when the time series is short or when there are missing observations.
CSS minimization was previously popular because methods for likelihood maximization were considered prohibitively slow, though this is no longer the case with currently available hardware and software \cite{ripley2002}.

For likelihood maximization, Eq.~\ref{eq:arma} is reformulated as an equivalent state-space model.
Although there are several ways this can be done, the approach of \cite{gardner1980} is widely used.
In this approach, we let $r = \max(p, q + 1)$ and extend the set of parameters so that $\allVarExt = \{\phi_1, \allowbreak \ldots, \allowbreak \phi_r, \allowbreak \theta_1, \allowbreak \ldots, \allowbreak \theta_{r-1}, \allowbreak \sigma^2_w\}$, with some of the $\phi_is$ or $\theta_is$ being equal to zero unless $p = q + 1$.
We define a latent state vector $z_t \in \mathbb{R}^{r}$, along with transition matrices $T\in \mathbb{R}^{r\times r}$ and $Q \in \mathbb{R}^{r\times 1}$, enabling the recovery of the original sequence $\{x_t\}$ using Equations~\ref{eq:trans} and \ref{eq:meas}.
For a detailed explanation of how to define the latent state $z_t$ and transition matrices $T$ and $Q$ in order to recover the ARMA model, we refer readers to Chapter~3 of Durbin and Koopman \cite{durbin12}.
Along with initializations for the mean and variance of $z_0$, these equations allow for the exact computation of the likelihood of the ARMA model via the Kalman filter \cite{kalman60}, which can subsequently be optimized by a numeric procedure such as the BFGS algorithm \cite{fletcher00}.
\begin{align}
  z_t &= Tz_{t - 1} + Qw_t, \label{eq:trans} \\
  x_t &= \begin{pmatrix} 1 & 0 & \ldots & 0\end{pmatrix} z_t. \label{eq:meas}
\end{align}

Numeric black-box optimizers require an initial guess for parameter values.
For ARMA models, this task is non-trivial because the valid parameter region is defined in terms of the roots of a polynomial associated with the parameters, as discussed in the next section.
The default strategy in R and Python is to use the CSS estimator for initialization.
This is an effective approach because the CSS estimator asymptotically converges to the MLE \cite{shumway2017}, and may therefore be close to the global maximum when there are sufficiently many observations.
However, the CSS initialization is less useful with limited data, or when there are missing observations.
The CSS estimate may also lie outside the valid parameter region, and in such cases, parameters are reinitialized at the origin.
Both software implementations also allow for manual selection of initial parameter values, but finding suitable initializations manually can be challenging due to complex parameter inter-dependencies.

The log-likelihood function of ARMA models is often multimodal \cite{ripley2002}, and therefore this single initialization approach can result in parameter estimates corresponding to local maxima (see Fig~\ref{fig:multiMode}).
This is true even for a carefully chosen initialization, such as the CSS estimate.
A common strategy to optimize multimodal loss functions is to perform multiple optimizations using different initial parameters.
However, we have found no instances of practitioners using a multiple initialization strategy for estimating ARMA model parameters.
This may be explained by a general unawareness of the possibility of converging to a local maximum or because obtaining a suitable collection of initializations for ARMA models is nontrivial.
For example, independently initializing parameters at random can place the parameter vector outside the region of interest.
Furthermore, uniform random sampling generally fails to adequately cover the plausible parameter region (\nameref{S_sampling}).

<<setupMulti>>=
sample_data <- function(data, job, n, P, Q, ...) {

  res <- list()

  coefs <- sample_ARMA_coef(
    order = c(P, Q), Mod_bounds = c(0.1, 0.9), min_inv_root_dist = 0.1
  ) |> round(digits = 2)

  res$ars <- coefs[grepl("^ar[[:digit:]]+", names(coefs))]
  res$mas <- coefs[grepl("^ma[[:digit:]]+", names(coefs))]

  res$x <- arima.sim(
    n = n,
    model = list(ma = res$mas)
  )

  res
}

checkImprove <- function(data, job, instance, ...) {

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0, coef = c('ma1' = 0))
  )

  c('num_starts' = arma2_fit$num_starts, 'll_improve' = ifelse(arma2_fit$num_starts > 10, arma2_fit$all_values[arma2_fit$num_starts] - arma2_fit$all_values[1], NA_real_), "truth" = instance$mas, "est" = arma2_fit$coef)
}

get_profile <- function(model, npts) {
  results <- matrix(ncol = 2, nrow = npts)

  fit_arma <- function(fixed) {

    P <- model$arma[1]
    Q <- model$arma[2]

    arima2::arima(
      model$x, order = c(0, 0, 1), fixed = c(fixed), SSinit = 'Rossignol2011',
      max_iters = 1, include.mean = FALSE
    )$loglik
  }

  fixed_vals <- seq(-1+1e-8, 1-1e-8, length.out = npts)
  lls <- purrr::map_dbl(fixed_vals, fit_arma)
  results[, 1] <- fixed_vals
  # results[, 2] <- model$coef[2]
  results[, 2] <- lls
  colnames(results) <- c(names(model$coef), 'loglik')
  as.data.frame(results)
}

ma1_res <- readRDS(paste0(root, "data/sim_MA1_results.rds"))

ma1_improve <- ma1_res %>%
  filter(num_starts > 10) %>%
  filter(abs(est.ma1) < 0.95)

exp_seed <- 100000
prob_seed <- 50000

# Loop --------------------------------------------------------------------

job_ids <- c(10463, 16385, 22383, 41307)
ggs <- list()

for (j in 1:length(job_ids)) {
  i <- job_ids[j]
  # i <- test_mods[j]
  set.seed(prob_seed + (i %% 25000) - 1)

  instance <- sample_data(
    NULL,
    i,
    pull(ma1_res, n)[i],
    P = 0, Q = 1
  )

  set.seed(exp_seed + i)

  arma2_fit <- tryCatch(
    arima2::arima(
      instance$x, order = c(0, 0, 1), SSinit = 'Rossignol2011', max_inv_root = 0.99,
      max_repeats = 10, include.mean = FALSE
    ),
    error = function(e) list(num_starts = 0)
  )

  stats1 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE
  )

  stats2 <- stats::arima(
    instance$x,
    order = c(0, 0, 1),
    SSinit = 'Rossignol2011',
    include.mean = FALSE,
    method = 'CSS'
  )

  mod_prof <- get_profile(arma2_fit, 1000)
  # mod_prof <- profile(arma2_fit, which = 1, lower = -1 + 1e-8, upper = 1 - 1e-8, npts = 1000)

  diffs <- case_when(
    j == 1 ~ 0.9,
    j == 2 ~ 3,
    j == 3 ~ 0.5,
    j == 4 ~ 0.5,
    TRUE ~ 2
  )

  df_lines <- data.frame(
    'value' = c(
      instance$mas,
      arma2_fit$coef[1],
      stats1$coef[1],
      stats2$coef[1]
    ),
    'type1' = c('truth', 'est', 'est', 'init'),
    'type2' = c('truth', 'est1', 'est2', 'truth')
  )

  gg_tmp <- mod_prof %>%
    filter(loglik > max(loglik) - diffs) %>%
    ggplot(aes(x = ma1, y = loglik)) +
    geom_line() +
    geom_vline(data = df_lines, aes(xintercept = value, col = type2, linetype = type1)) +
    geom_vline(xintercept = instance$mas, col = 'black') +
    geom_vline(xintercept = stats2$coef, col = 'black', linetype = 'dotted') +
    theme_bw() +
    labs(
      x = TeX('$\\theta_1$'),
      y = "Loglikelihood"
    ) +
    theme(plot.title = element_text(hjust = 0.5)) +
    # Add manual legend using dummy data
    scale_linetype_manual(
      name = NULL,
      values = c(
        "truth" = "solid",
        "est" = "dashed",
        "init" = "dotted"
      ), labels = c("truth" = 'Truth', "est" = "Estimate", "init" = 'CSS Initialization')
    ) +
    scale_color_manual(
      name = NULL,
      values = setNames(c("blue", "red", "black"), c("est1", "est2")),
      labels = setNames(c("arima2::arima", "stats::arima"), c("est1", "est2"))
    ) +
    guides(
      linetype = guide_legend(order = 1),
      color = guide_legend(order = 2)
    ) +
    theme(legend.title = element_blank(), legend.margin = margin(c(-2,0,0,0)),
          legend.key.spacing.y = unit(10, "pt"), legend.spacing.y = unit(35, 'pt'))

  if (j == 2 | j == 4) {
    gg_tmp <- gg_tmp + theme(axis.title.y = element_blank())
  }

  if (j == 1 | j == 2) {
    gg_tmp <- gg_tmp + theme(axis.title.x = element_blank())
  }

  ggs[[j]] <- gg_tmp
}
@


\begin{figure}[ht]
<<mutliMode-fig, fig.height=3.5, fig.width=7>>=
ggpubr::ggarrange(
  plotlist = ggs,
  legend = 'right',
  common.legend = TRUE
)
@
\caption{\label{fig:multiMode}The profile log-likelihood of data simulated from four distinct MA(1) models, demonstrating a few examples of multimodal likelihood surfaces. The solid, black line indicates the true value of $\theta_1$; the dotted line is the CSS-initialization. The dashed lines correspond to the estimate $\hat{\theta}_1$ using \code{stats:arima} (red) and our proposed algorithm (implemented in \code{arima2::arima}, blue).}
\end{figure}

\subsection{A Novel Multi-start Algorithm}

To obtain random parameter initializations, parameter sets must
correspond to \emph{causal} and \emph{invertible} ARMA processes;
definitions are in Chapter~3 of Shumway and Stoffer \cite{shumway2017}.
Let $\{\phi_i\}_{i = 1}^p$ and $\{\theta_i\}_{i = 1}^q$ be the coefficients of the $\text{ARMA}(p, q)$ model (Eq.~\ref{eq:arma}), and define $\Phi(x) = 1 - \phi_1 x - \phi_2x^2 - \ldots - \phi_px^p$ as the AR polynomial, and $\Theta(x) = 1 + \theta_1 x + \theta_2x^2 + \ldots + \theta_qx^q$ as the MA polynomial.
An ARMA model is \emph{causal} and \emph{invertible} if the roots of the AR and MA polynomials lie outside the complex unit circle.
Therefore in order to obtain valid, random parameter initializations, we sample the roots of $\Phi(x)$ and $\Theta(x)$ and use these values to reconstruct parameter initializations.

It is an easier task to sample \emph{inverted} roots, as the sufficient conditions for causality and invertibility now require that the inverted roots lie inside the complex unit circle, a region easier to sample uniformly.
The roots of the polynomials can be real or complex; complex roots must come in complex conjugate pairs in order for all of the corresponding model parameters to be real.
The simplest approach would be to sample all inverted root pairs ($z_1, z_2$) within the complex unit circle by uniformly sampling angles and radii (lines 10-14 of Algorithm~\ref{alg:mle}).
However, this would imply almost surely all root pairs are complex, and some model parameters would only be sampled as positive (or negative).
For instance, consider an AR(2) model.
The AR polynomial is:
$$
\Phi(x) = 1-\phi_1x - \phi_2x^2 = (1 - z_1x)(1 - z_2x) = 1 - (z_1 + z_2)x - (z_1z_2)x^2
$$
In this equation, if both $z_1, z_2$ are complex conjugates, then $\phi_2 = z_1z_2 > 0$.
As such, the only way that $\phi_2 < 0$ is if $z_1, z_2 \in \mathbb{R}$.
Similar results hold for the MA coefficients with opposite signs for the coefficients.
This issue is directly addressed in lines 5-8 of Algorithm~\ref{alg:mle}:
root pairs are sampled as real with probability $p = \sqrt{1/2}$, and real pairs sampled with the same sign with probability $p$, such that the product (and sums) of each pair is positive with probability $1/2$.
We sample conjugate pairs within an annular disk on the complex plane to avoid trivial and approximately non-stationary cases.
The radii of both the inner and outer circles defining the disk are defined using $\gamma$ in lines 7, 10, and 13 of Algorithm~\ref{alg:mle}.

\begin{algorithm}[ht]
    \SetAlgoNoEnd
   \caption[ARMA MLE]{\textbf{MLE for ARMA Models}. \\
    {\bf Inputs (defaults)}:\\\hspace{\textwidth}
    First parameter initialization $\psi_0 = (\phi^0_1, \ldots, \phi^0_p, \theta^0_1, \ldots, \theta^0_q)$ (CSS estimate).\\\hspace{\textwidth}
    Minimum acceptable polynomial root distance $\alpha > 0$, ($\alpha = 0.01$).\\\hspace{\textwidth}
    Probability of sampling a root pairs as real $0 \leq p \leq 1$, ($p = \sqrt{1/2}$).\\\hspace{\textwidth}
    Bounds on inverted polynomial roots $\gamma \in (0, 0.5)$, $(\gamma = 0.05)$.\\\hspace{\textwidth}
    Numeric optimization routine $f(\psi)$ \cite{gardner1980}.
    \\\hspace{\textwidth}
    Stopping Criterion (stop if last $M$ iterations do not improve log-likelihood $\ell(\psi)$).
    \label{alg:mle}}
Get preliminary estimate: $\hat{\psi}_0 = f(\psi_0)$; set $k = 0$\;
\Repeat( Until stopping criterion met){}{
  Set AR and MA roots $\{z^{\text{AR}}\}_{i = 1}^p = 0_p$, $\{z^{\text{MA}}\}_{i = 1}^q = 0_q$; increment $k$\;
  \While{$\min_{i, j}|z^{\text{AR}}_i - z^{\text{MA}}_j| < \alpha$, for both AR and MA polynomials} {
  Sample paired roots as real with probability $p$\;
    \For{all real pairs} {
      Sample root magnitudes from $U(\gamma, 1-\gamma)$\;
      Sample signs with
      $P(\text{sign}(z_1) = \text{sign}(z_2)) = p$\;
    }
    \For{all complex pairs}{
      sample angle: $\tau \sim U(0, \pi)$; sample radius: $r \sim U(\gamma, 1-\gamma)$\;
      set $z_{1} = r \cos(\tau) + i r \sin(\tau)$; set $z_{2} = \bar{z}_{1}$\;
    }  % End t loop
    \uIf{Number of roots is odd (non-paired root)}{
    sample $\tau$ uniformly from the set $\{0, \pi\}$; sample $r \sim U(\gamma, 1-\gamma)$\;
    set $z = r \cos(\tau)$\;
    }  % End if statement
  }  % End While

  Calculate coefficients $\psi_k = (\phi^k_1, \ldots,\phi^k_p, \theta^k_1, \ldots, \theta^k_q)$ using sampled roots\;
  Estimate $\hat{\psi}_k = f(\psi_k)$\;
}  % End k loop
  Set $\hat{\psi} = \argmax_{j\in 0:k} \ell\big(\hat{\psi}_j\big)$\;
\end{algorithm}

Parameter redundancy in ARMA models occurs when the polynomials $\Phi(x)$ and $\Theta(x)$ share one or more roots, leading to an overall reduction in model order.
This complicates parameter initialization, optimization, and identifiability.
The ARMA model (Eq.~\ref{eq:arma}) can be rewritten as:
\begin{equation}
  \Phi(B)x_t = \Theta(B)w_t, \label{eq:armaB}
\end{equation}
where $B$ is the \emph{backshift} operator, i.e., $Bx_t = x_{t - 1}$.
Using the fundamental theorem of algebra, Equation \ref{eq:armaB} can be factored into 
$$
(1 - \lambda_1B) \ldots (1 - \lambda_pB)x_t = (1 - \nu_1B) \ldots (1 - \nu_qB)w_t,
$$ 
where $\{\lambda_i\}_{i = 1}^p$ and $\{\nu_j\}_{j = 1}^q$ are the inverted roots of $\Phi(B)$ and $\Theta(B)$, respectively.
If $\lambda_i = \nu_j$ for any $(i, j) \in \{1, \ldots, p\} \times \{1, \ldots, q\}$, then the roots will cancel each other out, resulting in an ARMA model of smaller order.
As an elementary example, consider the $\text{ARMA}(1, 1)$ and $\text{ARMA}(2, 2)$ models in equations~\ref{eq:arma11} and \ref{eq:arma22}.
\begin{align}
  x_t &= \frac{1}{3}x_{t - 1} + w_t + \frac{2}{3}w_{t - 1}, \label{eq:arma11}\\
  x_t &= \frac{5}{6}x_{t - 1} - \frac{1}{6}x_{t - 2} + w_t + \frac{1}{6}w_{t - 1} - \frac{1}{3}w_{t - 2}. \label{eq:arma22}
\end{align}
While these two models appear distinct at first glance, re-writing the models in polynomial form (Eq.~\ref{eq:armaB}) shows that these two models are actually equivalent after canceling out the common factors on each side of the equation.

In a similar fashion, it is possible that the roots are not exactly equal but are approximately equal.
In this case, the ratio of factors becomes close to one, resulting in a similar effect to when the roots exactly cancel.
We avoid the possibility of \emph{nearly canceling roots} in parameter initializations by requiring the minimum Euclidean distance between inverted polynomial roots to be greater than $\alpha$.
This is done in line~4 of Algorithm~\ref{alg:mle}, though the condition is rarely triggered if the order of the model is of typical size $(p, q < 4)$.

Our sampling scheme is combined with existing procedures for numeric optimization of model log-likelihoods (lines 1 and 16), as well as the default initialization strategy for $\psi_0$ used by existing software (such as the CSS initialization).
Doing so guarantees that final estimates correspond to likelihood values greater than or equal to the currently accepted standards in the software environment where the algorithm is implemented.
For this article, both the numeric optimization procedure $f(\cdot)$ and the parameter initialization strategy to obtain $\psi_0$ are those implemented in \code{stats::arima}.
The stopping criterion was chosen so that the algorithm stops trying new initial values when no new maximum has been found using the last $M$ parameter initializations.
Alternative stopping criterion can be used (see for example, \cite{hart98}), but we found that this simple heuristic works well in practice.

Algorithm~\ref{alg:mle} is implemented in the R package \code{arima2}, available on the Comprehensive R Archive Network (CRAN) \cite{arima2Cran}.
The package features the function \code{arima2::arima}, which is an adaptation of the \code{stats::arima} function modified to incorporate the adjustments specified by Algorithm~\ref{alg:mle}.

\subsection{Simulation Studies}\label{sec:sims}

<<loadSimulation>>=
simulation1 <- readRDS(paste0(root, "data/sim1_results.rds"))

cat(paste0(root, "data/sim1_results.rds"))

results_stats <- simulation1 %>%
  filter(algorithm == 'statsArima')

results_arima2 <- simulation1 %>%
  filter(algorithm == 'Arima2') %>%
  mutate(job.id = job.id - 36000)

wide_res <- dplyr::full_join(
  x = results_stats,
  y = results_arima2,
  by = c("job.id"),
  suffix = c("_arima", "_arima2")
) %>%
  select(
    job.id, P = P_arima, Q = Q_arima, n = n_arima, ll_arima,
    ll_arima2, err_arima, err_arima2, num_restarts_arima,
    num_restarts_arima2, n_covered_arima, n_covered_arima2,
    n_prof_covered_arima, n_prof_covered_arima2,
    n_mle_failures_arima, n_mle_failures_arima2, n_prof_failures_arima,
    n_prof_failures_arima2
  )

tot_pct <- wide_res %>%
  filter(!is.na(ll_arima2) & !is.na(ll_arima)) %>%
  summarize(pct = sum(ll_arima2 > ll_arima) / n()) %>%
  as.numeric()

arma33_n50_pct <- wide_res %>%
  filter(n == 50, P == 3, Q == 3) %>%
  mutate(ll_diff = ll_arima2 - ll_arima) %>%
  summarize(pct = sum(ll_diff > 0, na.rm = TRUE) / sum(!is.na(ll_arima2) & !is.na(ll_arima))) %>%
  as.numeric()

ARMA_pct <- wide_res %>%
  filter(!is.na(ll_arima2) & !is.na(ll_arima)) %>%
  filter(P != 0 & Q != 0) %>%
  summarize(pct = sum(ll_arima2 > ll_arima) / n()) %>%
  as.numeric()

diff_IQR <- wide_res %>%
  filter(ll_arima2 > ll_arima) %>%
  mutate(ll_diff = ll_arima2 - ll_arima) %>% pull(ll_diff) %>% quantile(probs = c(0.25, 0.75))

median_diff <- wide_res %>%
  filter(ll_arima2 > ll_arima) %>%
  mutate(ll_diff = ll_arima2 - ll_arima) %>% pull(ll_diff) %>%
  median()

mean_restarts <- wide_res %>%
  filter(ll_arima2 > ll_arima) %>%
  pull(num_restarts_arima2) %>%
  mean()
@

To investigate the extent to which the standard approach for ARMA parameter estimation results in improperly maximized likelihoods, we conduct a series of simulation studies.
It is challenging to obtain precise estimates of how frequently current standards lead to sub-optimal parameter estimates due to the varied applications of ARMA models in practice, the diversity in data sizes ($n$) and model orders $(p, q)$, and the differing degrees to which an ARMA model adequately describes the data-generating process.
Therefore, we restrict our simulation studies to idealized scenarios where the data-generating process is Gaussian-ARMA, recognizing that likelihood maximization is easiest for this model class, thereby resulting in conservative estimates of how frequently our algorithm improves model likelihood.

In the first simulation study, we simulate time series data of lengths $n \in \{50, 100, 500, 1000\}$ from Gaussian-ARMA models with known orders $(p, q) \in \{1, 2, 3\}^2$, generating 36,000 unique models and datasets.
We avoid any models that contain parameter redundancies by requiring the data generating model to have a minimum distance of $0.1$ between all roots of $\Phi(x)$ and $\Theta(x)$.
We further restrict model coefficients so that they do not lie near boundary conditions.
Models of the same order of the generating data are fit to the data, simplifying the the problem further by avoiding the order selection step that is necessary in most data analyses.
In doing so, we attempt to answer the question of how often sub-optimal estimates may arise using existing software in the case where the parameter estimation procedure should be as easy as possible for the given combinations of $(n, p, q)$.

Even in this extremely simplified scenario, existing software failed to properly maximize model likelihoods in at least $\Sexpr{round(100 * tot_pct, 1)}\%$ of the simulated datasets---evidenced by an improvement obtained using Algorithm~1.
Though this improvement may appear modest, an improvement in $\Sexpr{round(100 * tot_pct, 1)}\%$ of the large number of published ARMA models would affect many papers---a number measured in thousands of papers since 2024 following our estimate in the introduction.
Furthermore, time series analysis courses and textbooks often recommend fitting multiple ARMA models to a dataset, and here we only fit one for each algorithm. 
Consequently, the probability that at least one candidate model is not properly optimized increases significantly in practice.
The rate of improvement obtained using our algorithm increases with model complexity and decreases with more observations (Fig.~\ref{fig:simProps}).
For example, likelihoods improved in $\Sexpr{round(100 * arma33_n50_pct, 1)}\%$ of the simulations when $p = q = 3$ and $n = 50$; models of this size and number of observations are not uncommon in published research studies.

<<getTiming>>=
sim_times <- readRDS(paste0(root, "data/sim1_timing.rds"))
mean_time <- sim_times$time.running %>% as.numeric() %>% mean()
@

\begin{figure}[!ht]
<<simProps, fig.height=3, fig.width=4.5>>=
wide_res %>%
  filter(P != 0 & Q !=0) %>%
  group_by(P, Q, n) %>%
  summarize(pct_improve = sum(ll_arima2 > ll_arima, na.rm = TRUE) / n()) %>%
  ggplot(aes(x = n, y = pct_improve)) +
  facet_grid(Q~P, labeller = labeller(P = AR_labs, Q = MA_labs)) +
  geom_line() +
  geom_point() +
  scale_y_continuous(limits = c(0, 0.6)) +
  ylab("Proportion of Improved Fits") +
  xlab("Number of observations (n)")
@
\caption{\label{fig:simProps}Proportion of simulated data with improved likelihood from using multiple restarts (Algorithm~\ref{alg:mle}).}
\end{figure}

Importantly, we do not claim to improve likelihood for all ARMA models and datasets.
In this simulation study, our likelihood maximum routine did not improve likelihoods for many of the simulated data.
However, these results demonstrate that there is a very real potential for obtaining sub-optimal parameter estimates when using only a single parameter initialization, even in the most idealistic scenarios.
Rather than having to worry if a single initialization is sufficient to fit a given model, it is preferable to adopt methods that make such situations rare.
The primary limitation of our algorithm is that the potential for improved fits comes at the cost of increased computation times.
In our simulation study, however, the average time to estimate parameters using our approach was $\Sexpr{round(mean_time, 1)}$ seconds, a computational expense that is worth the effort in many situations.

The median log-likelihood improvement in this simulation study was $\Sexpr{round(median_diff, 2)}$, with an interquartile range of $\Sexpr{paste0("(", paste0(round(diff_IQR, 2), collapse = ', '), ")")}$.
Among the most common motivations for fitting ARMA models is to model serial correlations in a regression model; in this setting, the discovered shortcomings in log-likelihood are often enough to change the outcome of the analysis.
For instance, consider modeling $y_i = \beta x_i + \epsilon_i$, where $\beta \in \mathbb{R}$, and we model the error terms $\epsilon_i \sim \text{ARMA}(p, q)$.
For now, we will assume the order $(p, q)$ is fixed.
We may wish to test the hypothesis $H_0: \beta = 0$ vs $H_1: \beta \neq 0$.
A standard approach to doing this is a likelihood ratio test, and using Wilks' theorem to get an approximate test.
We denote $ll_0$ and $ll_1$ as the maximum log-likelihood of the model under $H_0$ and $H_1$, respectively.
The standard approximation is to assume $2\Delta = 2(ll_1 - ll_0) \sim \chi^2_1$.
Using a significance level of $\alpha = 0.05$, we would reject $H_0$ if $\Delta \geq 1.92$.
Given that $E_{H_0}[\Delta] = 0.5$, subtracting the reported log-likelihood deficiencies (which has a median value of $\Sexpr{round(median_diff, 2)}$) of existing software to either or both $ll_0, ll_1$ could change the outcome of this test.

\subsubsection{Parameter Uncertainty}

Improved parameter estimation leads to modified standard error estimates, which are default outputs in R and Python.
These standard errors result from the numeric optimizer's estimate of the gradient of the log-likelihood, used to approximate the Fisher information matrix.
These standard errors, though not inherently of interest, are sometimes used justify the inclusion of a parameter in a model \cite[Chatper~9]{brockwell1991}.
We extend our simulation study to examine this approach and how our algorithm impacts the estimates.
For each of the 36,000 generative models from the previous study, we generate 100 additional datasets, estimating the MLE and Bonferroni-adjusted $95\%$ confidence intervals using both estimated standard errors and profile likelihood confidence intervals (PLCIs) from Wilks' theorem.
Fig~\ref{fig:pctCovered} shows PLCIs had better or equivalent nominal coverage than Fisher-based confidence intervals across all combinations of $p$, $q$, and $n$. We found that the interval estimation method mattered more for confidence interval performance than the specific parameter estimation algorithm.
The relevance of this result is explored further in Section~\ref{sec:HuronCI}.

\begin{figure}[!ht]
<<pctCovered, fig.height=4.2, fig.width=5.5>>=
gg_fisher <- simulation1 %>%
  filter(algorithm == "Arima2") %>%
  filter(P > 0 & Q > 0) %>%
  filter(n_mle_failures < 5, n_prof_failures < 5) %>%
  mutate(pct = n_covered / 100) %>%
  ggplot(aes(x = as.factor(n), y = pct, group = as.factor(n))) +
  geom_boxplot(outlier.stroke = 0.05) +
  facet_grid(P~Q, labeller = labeller(P = AR_labs, Q = MA_labs)) +
  geom_hline(yintercept = 0.95, linetype = 'dashed') +
  ylab("Coverage Proportion") +
  xlab("Number of Observations (n)") +
  theme(
    legend.title = element_blank(), legend.position = 'bottom',
    strip.background.y = element_blank(),
    strip.text.y = element_blank(),
    plot.margin = margin(20, 2, 5.5, 5.5),
    axis.text.y = element_text(size = 8),
    axis.text.x = element_text(size = 7)
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

gg_profile <- simulation1 %>%
  filter(algorithm == "Arima2") %>%
  filter(P > 0 & Q > 0) %>%
  filter(n_mle_failures < 5, n_prof_failures < 5) %>%
  mutate(pct = n_prof_covered / 100) %>%
  ggplot(aes(x = as.factor(n), y = pct, group = as.factor(n))) +
  geom_boxplot(outlier.stroke = 0.05) +
  facet_grid(P~Q, labeller = labeller(P = AR_labs, Q = MA_labs)) +
  geom_hline(yintercept = 0.95, linetype = 'dashed') +
  ylab("Coverage Proportion") +
  xlab("Number of Observations (n)") +
  theme(
    legend.position = 'none',
    axis.text.y = element_blank(), axis.title.y = element_blank(),
    axis.ticks.y = element_blank(),
    plot.margin = margin(20, 5.5, 5.5, 2),
    axis.text.x = element_text(size = 7)
  ) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, 0.25)) +
  scale_x_discrete(guide = guide_axis(n.dodge = 2))

ggpubr::ggarrange(
  gg_fisher, gg_profile, align = 'h', nrow = 1,
  labels = 'AUTO',
  label.x = c(0.16, 0)
)
@
\caption{\label{fig:pctCovered}Proportion of models that achieved nominal coverage of Bonferroni adjusted $95\%$ confidence intervals. The dashed line denotes the target coverage level. (A) Confidence intervals created using Fisher's information matrix. (B) Confidence intervals created using profile likelihoods.}
\end{figure}

\subsubsection{AIC Table Consistency}

A more realistic situation than the previous simulation study involves estimating the model order $(p, q)$ as well as obtaining parameter estimates.
Fitting multiple models raises the chance that at least one candidate model was not properly optimized.
It may also necessitate fitting larger models than needed, leading to parameter redundancies that make proper optimization more challenging.

A contemporary approach involves fitting several candidate models and selecting the one that minimizes a criterion like Akaike's information criterion (AIC) \cite{aic74}. 
This can be done by explicitly creating a table of all candidate models and their corresponding AIC values;
in this case issues of improper maximization become more apparent.
For instance, a table of AIC values may contain numeric inconsistencies, where a larger model may have lower estimated likelihoods than a smaller model within which it is nested (for an example, see Section~\ref{sec:depths}).
This type of result can make a careful practitioner feel uneasy, as there is evidence that at least one candidate model was not properly optimized.
Evidence of improper optimization may be less evident when relying on software that automates this process, such as the automated Hyndman-Khandakar algorithm \cite{hyndman08}, but the potential for sub-optimal estimates remains.

<<simTables>>=
simTables <- readRDS(paste0(root, "data/sim2_AICtable_consistency.rds"))
arma_inc <- simTables %>%
  filter(max_repeat == 1) %>%
  summarize(1 - sum(consistent, na.rm = TRUE) / n()) %>%
  unlist()
@

We conducted an additional simulation study to investigate numeric inconsistencies that may arise when fitting multiple model parameters.
As before, we simulated 1000 unique models and datasets of size $n \in \{50, 100, 500, 1000\}$ from Gaussian ARMA$(p, q)$ models for $(p, q) \in \{1, 2, 3\}^2$.
To avoid models with parameter redundancies, we ensured a minimum distance of 0.1 between all roots of $\Phi(x)$ and $\Theta(x)$ and excluded models with coefficients near boundary conditions.
For each dataset, AIC tables were created for model sizes $(p, q) \in \{0, 1, 2, 3\}^2$.

The single parameter initialization approach resulted in AIC table inconsistencies in $\Sexpr{round(100*arma_inc, 1)}\%$ of the simulated datasets.
Although our proposed algorithm significantly mitigates this issue, it does not guarantee that all model likelihoods are fully maximized.
This is illustrated in Fig~\ref{fig:AICtabRes}, where a non-zero percentage of AIC tables remain inconsistent, even as the algorithm's stopping criterion grows.
The ARMA$(1, 1)$ panel in Fig~\ref{fig:AICtabRes} illustrates the increasing difficulty of parameter estimation when dealing with parameter redundancies.
In such cases, it is often necessary to adjust additional parameters in the numeric optimization routine.
For example, our R implementation of the algorithm relies on the generic BFGS optimizer in the \code{stats::optim} function.
Modifying the optimization method or the default hyperparameters can lead to improved fits or faster convergence rates.
While the default parameters of the numeric optimizer are generally adequate, increasing the maximum number of algorithmic iterations can be beneficial for fully maximizing the likelihood for challenging models and data. 

\begin{figure}[!ht]
<<AICtabRes, fig.height=4.25, fig.width=6>>=
restart_labels <- c(
 '1'  = "M = 1",
 '10' = "M = 10",
 '30' = "M = 30",
 '50' = "M = 50"
)

simTables %>%
  group_by(n, P, Q, max_repeat) %>%
  summarize(
    pct_consistent = sum(consistent, na.rm = TRUE) / sum(!is.na(consistent))
  ) %>%
  ggplot(
    aes(
      x = n, y = pct_consistent,
      group = as.factor(max_repeat),
      color = as.factor(max_repeat),
      linetype = as.factor(max_repeat)
    )
  ) +
  geom_line() +
  geom_point() +
  facet_grid(P~Q, labeller = labeller(P = AR_labs, Q = MA_labs)) +
  ylab("Proportion of Tables\n that are Consistent") +
  xlab("Number of observations") +
  theme(legend.title = element_blank(), legend.position = 'bottom') +
  scale_color_discrete(labels = restart_labels) +
  scale_linetype_discrete(labels = restart_labels) +
  scale_y_continuous(limits = c(0.35, 1))
@
\caption{\label{fig:AICtabRes}Data is generated from ARMA$(p, q)$ models with $(p, q) \in \{1, 2, 3\}^2$, and the corresponding AIC table is created. The Y-axis shows the percentage of tables that were consistent. M is the number of times a maxima is observed before the algorithm terminates, so $M=1$ corresponds to the standard maximization procedure.}
\end{figure}

The contemporary approach of using AIC---or any other information based criteria---to select model order involves fitting unnecessarily large models, leading to parameter redundancies that complicate likelihood optimization.
The use of AIC for ARMA model order selection has theoretical support, particularly for forecasting, as ARMA models inspired the original AIC paper \cite{aic74}.
However, without proper likelihood maximization, a strategy that considers only a single parameter initialization may not truly minimize AIC.
In this framework, likelihood maximization and over-parameterization are interconnected: all candidate models must be maximized for likelihood, or users risk selecting over-parameterized models that fail to minimize the intended information criterion.
For the current study, the choice of AIC versus other popular information criteria such as the corrected AIC (AICC) or Bayesian information criterion (BIC) is unimportant: all of these approaches rely on proper optimization of the likelihood function, which is the problem we are addressing here. 

Classical ARMA modeling addresses this by recommending diagnostic plots to determine appropriate model order and advising against simultaneously adding AR and MA components.
In this approach, the additional difficulty in parameter estimation associated with fitting models containing parameter redundancies is avoided by not fitting overly complex models when possible.
Despite this, shortcomings in likelihood maximization can occur even in models without parameter redundancies (Figs~\ref{fig:multiMode},~\ref{fig:simProps},~and~\ref{fig:AICtabRes}), necessitating the exploration of multiple parameter initializations.
Further, the increasing preference of using automated software to pick the model size using an information criterion suggests the importance of using software that reliably maximizes model likelihoods even in the presence of over-parameterization. 

\section{Annual Depths of Lake Michigan}\label{sec:depths}

\noindent In this example, we illustrate how improperly maximized likelihoods can lead to inconsistencies and uncertainty in a real data analysis scenario.
Additionally, we show how the common practice of using the estimated standard error for calibrated parameters can misleadingly support the inclusion of model parameters.
We consider a dataset containing annual observations on the average depth of Lake Michigan-Huron, recorded the first day of each year from 1860-2014 (Fig~\ref{fig:huron}) \cite{NOAA16}.
We wish to develop an ARMA model for these data, which is a standard task in time series analysis \cite{shumway2017}.

\begin{figure}[!ht]
<<huron, fig.height=2.5,fig.width=4.5>>=
data("miHuron_level")
ggplot(miHuron_level, aes(x = Date, y = Average)) +
  geom_line() +
  ylab("Average Lake Depth (ft)") +
  theme(axis.title.x = element_blank())
@
\caption{\label{fig:huron}
Average depth of Lake Michigan-Huron from 1860-2014.
}
\end{figure}

Diagnostic tests, such as sample autocorrelation and normal quantile plots for residuals, suggest that it is reasonable to model the data in Fig~\ref{fig:huron} as a weakly stationary Gaussian $\text{ARMA}(p, q)$ process for some non-negative integers $p$ and $q$.
While an ARIMA model may also be reasonable for these data, we first consider fitting an ARMA model because we would like to avoid the possibility of over-differencing the data.
The next step is to determine appropriate values of $p$ and $q$;
after some initial investigation, multiple combinations of $p$ and $q$ seem plausible, and therefore we decide to choose the values of $p$ and $q$ that minimize the AIC.
For simplicity, we create a table of AIC values for all possible combinations of $(p, q) \in \{0, 1, 2, 3\}^2$ (Table~\ref{tab:huronTab}).
Using the AIC as the model selection criterion, the selected model size is $\text{ARMA}(2, 1)$.

<<getTableValues, include=FALSE, warning=FALSE, cache=TRUE>>=
set.seed(12345)
P <- 3
Q <- 3
tab_tol <- 1e-2
huron_tab_vals <- aicTable(
  miHuron_level$Average, P = P, Q = Q, max_iters = 1,
  optim.control = list(maxit = 1000)
)
huron_tab_vals2 <- aicTable(
  miHuron_level$Average, P = P, Q = Q,
  optim.control = list(maxit = 1000)
)

is_improved <- huron_tab_vals2 + tab_tol < huron_tab_vals
@

\begin{table}[h]
    \centering
    % Start the first minipage for Table 1
    \begin{subtable}{0.4\textwidth}
        \centering
        \caption{\label{tab:AIC1}Single parameter initialization.}
        \begin{tabular}{c|c|c|c|c}\hline
            & MA0 & MA1 & MA2 & MA3 \\
            \hline
            AR0 & \Sexpr{ifelse(is_improved[1, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[1, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[1, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[1, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[1, 4], 1), nsmall = 1)} \\
            \hline
            AR1 & \Sexpr{ifelse(is_improved[2, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[2, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[2, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[2, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[2, 4], 1), nsmall = 1)} \\
            \hline
            AR2 & \Sexpr{ifelse(is_improved[3, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[3, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[3, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[3, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[3, 4], 1), nsmall = 1)} \\
            \hline
            AR3 & \Sexpr{ifelse(is_improved[4, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[4, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[4, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[4, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals[4, 4], 1), nsmall = 1)} \\
            \hline
        \end{tabular}
    \end{subtable}
    \hspace{15mm} % this fills the space between minipages if needed
    % Start the second minipage for Table 2
        \begin{subtable}{0.4\textwidth}
        \centering
        \caption{\label{tab:AIC2}Multiple parameter initializations.}
        \begin{tabular}{c|c|c|c|c}\hline
            & MA0 & MA1 & MA2 & MA3 \\
            \hline
            AR0 & \Sexpr{ifelse(is_improved[1, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[1, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[1, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[1, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[1, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[1, 4], 1), nsmall = 1)} \\
            \hline
            AR1 & \Sexpr{ifelse(is_improved[2, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[2, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[2, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[2, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[2, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[2, 4], 1), nsmall = 1)} \\
            \hline
            AR2 & \Sexpr{ifelse(is_improved[3, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[3, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[3, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[3, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[3, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[3, 4], 1), nsmall = 1)} \\
            \hline
            AR3 & \Sexpr{ifelse(is_improved[4, 1], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[4, 1], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 2], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[4, 2], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 3], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[4, 3], 1), nsmall = 1)} & \Sexpr{ifelse(is_improved[4, 4], "\\improvedCell", "")} \Sexpr{format(round(huron_tab_vals2[4, 4], 1), nsmall = 1)} \\
            \hline
        \end{tabular}
    \end{subtable}
    \caption{\label{tab:huronTab}AIC values for an ARMA(p, q) model fit to Lake Michigan-Huron depths. Table~\ref{tab:AIC1} was computing using only a single parameter initialization. Table~\ref{tab:AIC2} was computed using Algorithm~\ref{alg:mle}. Highlighted cells show where the likelihood was improved (AIC reduced) using our algorithm.}
\end{table}

<<huronTab-latex, include=FALSE, eval=FALSE>>=
round(huron_tab_vals, 1) %>%
  knitr::kable(
    format = 'latex',
    caption = 'AIC values for an ARMA(p, q) model fit to Lake Michigan-Huron depths.',
    label = "huronTab"
  )
@

Recall that the AIC is defined as:
\begin{equation}
  \text{AIC} = -2 \max_{\allVar} \ell(\allVar; x^*) + 2d,\label{eq:AIC}
\end{equation}
where $\ell(\allVar; x^*)$ denotes the log-likelihood of a model indexed by parameter vector $\allVar \in \mathbb{R}^d$, $d \geq 1$, given the observed data $x^*$.
In the case of an ARMA model with an intercept, $d = p + q + 2$, where the additional parameter corresponds to a variance estimate.
If either $p$ or $q$ increases by one, then a corresponding increase in AIC values greater than two suggests that the \emph{inclusion} of an additional parameter resulted in a \emph{decrease} in the maximum of the log-likelihood, which is mathematically impossible under proper optimization.
Several such cases are present in Table~\ref{tab:AIC1}, for example increasing from an $\text{ARMA}(2, 2)$ model to a $\text{ARMA}(3, 2)$ model results in
a decrease of \Sexpr{format(round((huron_tab_vals['AR3', 'MA2'] - huron_tab_vals['AR2', 'MA2'] - 2) / 2, 1), nsmall = 1)} log-likelihood units.
In this case, using our multiple restart algorithm eliminates all instances of mathematical inconsistencies (Table~\ref{tab:AIC2}).
We refer to tables that have log-likelihood values larger for any smaller nested model within the table as \emph{inconsistent}.

Suppose a scientist is confronted with a mathematically implausible table of nominally maximized likelihoods (Table~\ref{tab:AIC1}).
How much should they worry about this?
Is it acceptable to publish scientific results that demonstrate a nominally maximized likelihood is not, in fact, maximized?
Can researchers confidently trust the scientific implications of a fitted model if there is evidence of improper optimization in some of the candidate models?
Given a choice, a researcher should prefer to use maximization algorithms reliable enough to make such situations rare.
In the Lake Michigan example, improved estimation does not change which model is selected or the final parameter estimates, but it does remove inconsistencies that could lead to these concerns (Table~\ref{tab:AIC2}).

Minimizing the AIC (or an alternative information criterion) is not the only accepted approach to order selection.
A classical perspective on model selection involves consulting sample autocorrelation plots, partial autocorrelation plots, conducting tests such as Ljung-Box over various lags, studying the polynomial roots of fitted models, and checking properties of the residuals of the fitted models \cite{box1970, brockwell1991, shumway2017}.
This approach helps avoid fitting models that are possibly over-parameterized.
However, additional computational power and increasing volumes of data have favored automated data analysis strategies that fit many models and evaluate them using a model selection criterion.
In principle, a simple model selection criterion such as AIC can address parsimony and guard against over-parameterization as well.
Diagnostic inspection can be combined with these automated approaches.
For example, a table of AIC values can be generated, and models with promising likelihoods can be explored further \cite{brockwell1991}.

When possible, there may be general agreement that the best approach is to combine modern computational resources with careful attention to model diagnostics, considering the data and the scientific task at hand.
Improved maximization facilitates this process by eliminating distractions resulting from incomplete maximization.

\subsection{Parameter uncertainty}\label{sec:HuronCI}

<<fitHuron21>>=
myround <- function(x, digits = 1) {
  # taken from the broman package
  if (digits < 1)
    stop("This is intended for the case digits >= 1.")
  if (length(digits) > 1) {
    digits <- digits[1]
    warning("Using only digits[1]")
  }
  tmp <- sprintf(paste("%.", digits, "f", sep = ""), x)
  zero <- paste0("0.", paste(rep("0", digits), collapse = ""))
  tmp[tmp == paste0("-", zero)] <- zero
  tmp
}
huron_21 <- arima2::arima(miHuron_level$Average, order = c(2, 0, 1))
@

Default output from fitting an ARMA model in R or Python includes estimates for parameter values and their standard errors, calculated using Fisher's information matrix.
If the ARMA model with the lowest AIC value is chosen to describe the Lake Michigan data, then an $\text{ARMA}(2, 1)$ model is selected.
The estimated coefficients and standard errors obtained after fitting this model are reported in Table~\ref{tab:huronCoefsTab}.
The small standard error for $\hat{\theta}_1$ reported in this table suggests a high-level of confidence that the parameter has a value near $1$.
Taken at face value, these estimates seem to strongly favor the inclusion of the MA(1) term in the model.

\begin{table}
\centering
\caption{\label{tab:huronCoefsTab}Parameter values of $\text{ARMA}(p, q)$ model fit to Lake Michigan-Huron depth data.}
\begin{tabular}{c|c|c|c|c}\hline
 & $\phi_1$ & $\phi_2$ & $\theta_1$ & Intercept \\\hline
 Estimate & \Sexpr{format(round(huron_21$coef['ar1'], 3), nsmall = 3)} & \Sexpr{format(round(huron_21$coef['ar2'], 3), nsmall = 3)} & \Sexpr{format(round(huron_21$coef['ma1'], 3), nsmall = 3)}  & \Sexpr{format(round(huron_21$coef['intercept'], 3), nsmall = 3)} \\\hline
 s.e. & \Sexpr{format(round(sqrt(diag(huron_21$var.coef))['ar1'], 3), nsmall = 3)} & \Sexpr{format(round(sqrt(diag(huron_21$var.coef))['ar2'], 3), nsmall = 3)} & \Sexpr{format(round(sqrt(diag(huron_21$var.coef))['ma1'], 3), nsmall = 3)}  & \Sexpr{format(round(sqrt(diag(huron_21$var.coef))['intercept'], 3), nsmall = 3)} \\\hline
\end{tabular}
\end{table}

<<huronCoefsTab, eval=FALSE, include=FALSE>>=
colnames(coefs_tab) <- c("", "$\\phi_1$", "$\\phi_2$", "$\\theta_1$", "intercept")
coefs_tab %>%
  knitr::kable(
    format = 'latex',
    caption = 'Parameter values of ARMA(p, q) model fit to Lake Michigan-Huron depth data.',
    label = 'huronCoefsTab',
    escape = FALSE
  )
@

However, our simulation studies have suggested that these confidence intervals can be misleading, and that PLCIs are more reliable alternatives.
The $95\%$ PLCI for the parameter (Fig~\ref{fig:huronMAevid}A) is much larger than the confidence interval created using these standard errors.
The steep curve in the immediate vicinity of $\hat{\theta}_1$ may explain the small standard error estimates for this parameter and the corresponding tight confidence intervals created using Fisher's identity matrix.
Alternative evidence indicates the potential for nearly canceling roots (Fig~\ref{fig:huronRoots}), in which case the MA(1) term may not be needed in the model.

<<ma1PCI>>=
prof <- profile(huron_21, which = 3, lower = -0.99, upper = 1.1, npts = 1000L) %>%
  mutate(
    inCI = ifelse(max(loglik, na.rm = TRUE) - loglik < 1.92, TRUE, FALSE)
  )

min_confidence <- prof %>%
  tidyr::drop_na() %>%
  filter(inCI) %>%
  filter(ma1 == min(ma1)) %>%
  pull(ma1)
@

<<arma21sims>>=
set.seed(34432)
registerDoRNG(448912)

theta_21 <- bake(
  file = paste0(root, 'data/PLCIsims.rds'),
  {

    J <- 1000
    params <- coef(huron_21)
    ar <- params[grep("^ar", names(params))]
    ma <- params[grep('^ma', names(params))]
    intercept <- params['intercept']
    sigma <- sqrt(huron_21$sigma2)

    theta_21 <- foreach(j=1:J, .combine = rbind) %dopar% {
      Y_j <- arima.sim(
        list(ar = ar, ma = ma),
        n = nrow(miHuron_level),
        sd = sigma
      ) + intercept


      tryCatch(
        coef(arima(Y_j, order = c(2, 0, 1), transform.pars = TRUE)),
        error = function(e) c("ar1" = NA, "ar2" = NA, "ma1" = NA, intercept = NA)
      )
    }

    as.data.frame(theta_21)
  }
)

lwr_prof <- prof %>%
  filter(inCI) %>%
  filter(ma1 == min(ma1)) %>%
  pull(ma1)

lwr_fisher <- huron_21$coef['ma1'] - 1.96 * sqrt(huron_21$var.coef['ma1', 'ma1'])

pct_sim_cov_PL <- sum(theta_21$ma1 >= lwr_prof) / nrow(theta_21)
pct_sim_cov_FI <- sum(theta_21$ma1 >= lwr_fisher) / nrow(theta_21)
@


<<calcHist21>>=
registerDoRNG(566223)

theta_1 <- bake(
  file = paste0(root, 'data/ar1sims.rds'),
  {

    J <- 1000
    huron_ar1 <- arima(miHuron_level$Average, order = c(1, 0, 0))
    params <- coef(huron_ar1)
    ar <- params[grep("^ar", names(params))]
    intercept <- params['intercept']
    sigma <- sqrt(huron_ar1$sigma2)

    theta_1 <- foreach(j=1:J, .combine = rbind) %dopar% {
      Y_j <- arima.sim(
        list(ar = ar),
        n = nrow(miHuron_level),
        sd = sigma
      ) + intercept

      tryCatch(
        coef(arima(Y_j, order = c(2, 0, 1), transform.pars = TRUE)),
        error = function(e) c("ar1" = NA, "ar2" = NA, "ma1" = NA, intercept = NA)
      )
    }

    as.data.frame(theta_1)
  }
)
@

\begin{figure}[!ht]
<<huronMAevid, fig.height=2.25>>=
gg_prof <- prof %>%
  tidyr::drop_na() %>%
  ggplot(aes(x = ma1, y = loglik, linetype = inCI, color = inCI)) +
  geom_line() +
  scale_color_manual(values = c("red", 'black'),
                     name = "In PLCI") +
  theme_bw() +
  ylab("log-likelihood") +
  xlab(latex2exp::TeX("$\\theta_1$")) +
  scale_linetype_manual(
    values = c(2, 1),
    name = "In PLCI"
  ) +
  geom_vline(xintercept = min_confidence, linetype = 'dotted') +
  theme(legend.position = 'none')

gg_ma21 <- ggplot(theta_21, aes(ma1)) +
  geom_histogram(breaks = seq(-1, 1, 0.1), col = 'black') +
  theme_bw() +
  xlab(latex2exp::TeX("$\\hat{\\theta}_1$")) +
  ylab("Count") +
  scale_x_continuous(breaks = seq(-2, 2, 0.5))

gg_ma1 <- ggplot(theta_1, aes(ma1)) +
  geom_histogram(breaks = seq(-1, 1, 0.1), col = 'black') +
  theme_bw() +
  xlab(latex2exp::TeX("$\\hat{\\theta}_1$")) +
  ylab("Count") +
  scale_x_continuous(breaks = seq(-2, 2, 0.5))

cowplot::plot_grid(gg_prof, gg_ma21, gg_ma1, nrow = 1, labels = 'AUTO')
@
\caption{\label{fig:huronMAevid}Evidence for an AR(1) model for the Lake Michigan-Huron data. (A) Profile likelihood confidence interval (PLCI) for $\theta_1$ which includes the value $\theta_1 = 0$. The vertical dotted line represents the lower end of the approximate confidence interval; all points on the solid black line lie within the confidence interval, and points on the dashed red line are outside the interval. (B) Histogram of re-estimated $\theta_1$ values using simulated data simulated from the $\text{ARMA}(2, 1)$ model that was calibrated to the Lake Michigan-Huron data. (C) Histogram of re-estimated $\theta_1$ values using data simulated from the AR(1) model that was calibrated to the Lake Michigan-Huron data.}
\end{figure}

\begin{figure}[!ht]
<<huronRoots, fig.width=6.7, fig.height=3.3>>=
set.seed(123)

mod_10 <- arima2::arima(
  miHuron_level$Average, order = c(1, 0, 0),
  max_iters = 1, SSinit = 'Rossignol2011'
)

roots10 <- data.frame(
  type = 'AR',
  real = Re(1/ARMApolyroots(mod_10)),
  im = Im(1/ARMApolyroots(mod_10)),
  mod = "AR(1)"
)

mod_21 <- arima2::arima(
  miHuron_level$Average, order = c(2, 0, 1),
  max_iters = 1, SSinit = 'Rossignol2011'
)

min_dist_21 <- min(Mod(outer(1/ARMApolyroots(mod_21), 1/ARMApolyroots(mod_21, type = 'MA'), FUN = '-')))

roots21 <- data.frame(
  type = c(rep('AR', length(ARMApolyroots(mod_21))), rep('MA', length(ARMApolyroots(mod_21, type = 'MA')))),
  real = c(Re(1/ARMApolyroots(mod_21)), Re(1/ARMApolyroots(mod_21, type = 'MA'))),
  im = c(Im(1/ARMApolyroots(mod_21)), Im(1/ARMApolyroots(mod_21, type = 'MA'))),
  mod = 'ARMA(2,1)'
)

all_roots <- dplyr::bind_rows(roots10, roots21)

ggplot(all_roots, aes(x = real, y = im, col = mod, pch = mod)) +
  annotate("path", x = cos(seq(0, 2*pi, length.out = 100)),
           y = sin(seq(0, 2*pi, length.out = 100))) +
  geom_hline(yintercept = 0, linewidth = 0.3) +
  geom_vline(xintercept = 0, linewidth = 0.3) +
  geom_point(aes(size = mod), stroke = 1.2) +
  facet_wrap(
    ~type,
    nrow = 1,
    labeller = as_labeller(
      c("AR" = "Inverse AR roots", "MA" = "Inverse MA roots"))
  ) +
  coord_fixed() +
  labs(
    color = NULL, pch = NULL, stroke = NULL, size = NULL, y = 'Imaginary', x = 'Real'
    ) +
  theme_bw() +
  theme(axis.title = ggplot2::element_text(face="bold"),
        legend.position = 'bottom') +
  scale_shape_manual(values = c(1, 4, 3)) +
  scale_size_manual(values = c(3.7, 2.7, 2.7)) +
  scale_color_brewer(palette = 'Dark2')
@
\caption{\label{fig:huronRoots}Inverted AR and MA polynomial roots to the fitted $\text{AR}(1)$ and $\text{ARMA}(2, 1)$ models to the Lake Michigan-Huron data using a single parameter initialization.}
\end{figure}

Both types of confidence intervals considered in this example rely on asymptotic justifications, but we can further investigate the finite sample properties using a simulation study.
We fit both $\text{ARMA}(2, 1)$ and $\text{AR}(1)$ models to the data, and conduct a boot-strap simulation study by simulating $1000$ datasets from each of the fitted models.
We then re-estimate an $\text{ARMA}(2, 1)$ model to each of these datasets and record the estimated coefficients.
A histogram containing the estimated values of $\hat{\theta}_1$ when the data are generated from the $\text{ARMA}(2, 1)$ and $\text{AR}(1)$ models fit to the Lake Huron-Michigan data are shown in Fig~\ref{fig:huronMAevid}B and Fig~\ref{fig:huronMAevid}C, respectively.

The shape of this histogram in Fig~\ref{fig:huronMAevid}B mimics that of the profile log-likelihood surface in Fig~\ref{fig:huronMAevid}A, confirming that a large confidence interval is needed in order to obtain a $95\%$ confidence interval.
In Fig~\ref{fig:huronMAevid}C, a large number of $\hat{\theta}_1$ coefficients are estimated near $1$ when the generating model is $\text{AR}(1)$.
Combining this result with the nearly canceling roots of the $\text{ARMA}(2, 1)$ model (Fig~\ref{fig:huronRoots}), we cannot reject the hypothesis that the data were generated from a $\text{AR(1)}$ model, even though the Fisher information standard errors suggest that the data should be modeled with a nonzero $\theta_1$ coefficient.

\section{Discussion}

\noindent A significant motivation for our work is the observation that commonly used statistical software that purports to maximize ARMA model likelihoods fails to do so for a large number of examples.
In addition to improving parameter estimates, proper maximization of ARMA model likelihoods is crucial because ARMA models are often used to model serial correlations in regression analyses.
In this context, researchers may perform likelihood ratio hypothesis tests for regression coefficients, and the validity of these tests depends on proper likelihood optimization.

An important consequence of improved likelihood maximization is better model selection.
A common approach to selecting an ARMA model involves fitting different sizes of models and choosing the one that minimizes an information criterion, such as the AIC.
Fitting multiple models results in having a higher probability that at least one candidate model was not properly maximized.
Since AIC assumes the parameters correspond to maximized likelihoods, enhancements in likelihood maximization can lead to different model selections.
Consequently, methodology relying on existing estimation methods---like the popular \code{auto.arima} function in the \code{forecast} package in R \cite{hyndman08}, which minimizes the AIC of a group of candidate models without explicitly displaying an AIC table---will be impacted by improved estimates.

Our proposed algorithm is supported by existing theory on likelihood evaluation of linear state-space models via the Kalman Filter \cite{kalman60}, the same as the current existing standard approach for parameter estimation.
The simulation studies that we have conducted, however, demonstrate the importance of considering multiple parameter initializations in order to fully maximize model likelihoods. 
These simulations provide a conservative estimate of how frequently our algorithm results in improved likelihoods compared to existing standards.
A common situation where our algorithm is expected to provide even larger improvements than those reported here is in the presence of missing data, a primary motivator of the likelihood maximization procedure of existing software \cite{ripley2002}.
In this situation, the well-informed CSS initialization is not available, and the default approach is to initialize at the origin, resulting in a greater need to attempt multiple parameter initializations.  

Parameter estimates corresponding to higher likelihood values are not necessarily scientifically preferrable to alternative regions of parameter space with lower likelihood values \cite{lecam1990}.
Sometimes, our improved estimates may result in models with nearly canceling roots, parameters near boundary conditions, or otherwise unfavorable statistical properties.
On other occasions, our method can rescue a naive optimization attempt from a local maximum having those unfavorable properties.
Practitioners should carefully evaluate fitted models to ensure they are appropriate for the data and problem at hand.

The primary limitation of our approach is that it achieves higher likelihoods at the cost of processing speed, which is more pronounced with large datasets.
However, our algorithm is most necessary for small datasets ($n \ll 10000$), where default parameter initialization strategies may perform poorly.
Therefore, our algorithm is most beneficial for small to moderate sample sizes, where the additional computational cost is generally negligible.
The compute time of our algorithm is approximately $K$ times slower than the default approach, where $K$ is the number of unique parameter initializations.
This is only an approximation of the actual additional cost as not all initializations require the same amount of processing time in order to converge.
In particular, initializations that are already close to local maximum will generally converge much quicker than those that are further away.

Our proposed algorithm for ARMA parameter estimation significantly advances statistical practice by addressing a frequently occurring optimization deficiency.
Because existing software can also be leveraged to mitigate the issue, the largest contribution of this work may be highlighting the prevalence of this optimization problem.
Traditional random initialization approaches software fail to uniformly cover the entire range of possible models and often produce many initializations outside the accepted range.
Our algorithm offers a computationally efficient and practically convenient solution, providing a robust approach to parameter initialization and estimation that ensures adequate coverage of all possible models.
We have shown that it provides a new standard for best practice in the field of time series analysis.

